{
  "generated_text": "## Motivation\n\nThe rapid advancement of large language models (LLMs) has brought significant capabilities but also the challenge of ensuring safety in various applications. Traditional adversarial attacks on LLMs have been limited to specific scenarios, such as jailbreak attempts and harmful content generation, and often involve subversive personas. Existing benchmarks and attack methods have not adequately addressed the need for more nuanced and realistic evaluations, particularly in controlling and measuring harmful content on chatbot applications. This research aims to highlight vulnerabilities in the control and monitoring of harmful content generation, emphasizing the need for improved benchmark datasets and attack/defense methods that go beyond simplistic measures.\n\n## Main Idea\n\nThe paper introduces Persona-Pair Adversarial Training (PPAT), a method to enhance the safety of LLMs by including diverse personas in pretraining and finetuning. The approach leverages knowledge from cognitive science to disambiguate persona dimensions such as power and privilege to increase the diversity of personas. It also proposes a novel adversarial training strategy where persona conditions are sampled to adversarially influence the model, unlike standard RLHF. The paper also creates a more realistic and disambiguated dataset, Anti-HARSH, for more effective evaluation of control measures in chatbots.\n\n## Interestingness\n\n8\n\n## Feasibility\n\n7\n\n## Novelty\n\n9\n\n```latex\n\\title{Leveraging Adversarial Personas to Enhance the Safety of Large Language Models}\n\n\\begin{abstract}\nEnsuring the safety of large language models (LLMs), such as ChatGPT or Llama, has become an important issue. However, most work has focused on latent and active attempts to produce harmful content, often in the context of jailbreak attacks. This work will propose a way to increase the diversity of personas, particularly those related to power, to use in adversarial training, and a way to adversarially influence Llama2. The paper also introduces a more realistic benchmark dataset for controlling and measuring harmful content in chatbots.\n\\end{abstract}\n\n\\section{Motivation}\n\n\n\nThe rapid advancement of large language models (LLMs) has brought significant capabilities but also the challenge of ensuring the safety of these models in various applications. By creating benchmarks and methods for adversarial attacks, researchers can better understand the vulnerabilities and strengths of LLMs. This paper focuses on designing attack methods for LLMs, specifically in the context of controlling and measuring harmful content generation in chatbots. Existing benchmarks and methods to evaluate attacks on LLMs have generally not been as realistic as needed to drive forward progress. For instance, we find that for the popular Llama2 model, simple jailbreak attacks can lead to significant amounts of harmful content generation, but these content types are not always desired by developers. There is a need for more nuanced attack and defense methods to better evaluate the effectiveness of control measures of harm in chatbots.\n\n\n```\n\n## Experimental Setup\n\n```json\n[{\"name\": \"Persona Control in Chatbot Applications\", }\n\n```json\n[{\"name\": \"Persona-Pair Adversarial Training\", \"description\": \"This experiment aims to enhance the safety of LLMs by finetuning them under persona conditions to increase the diversity of personas in the model. The method uses cognitive science knowledge to define prosocial and antisocial personas along various dimensions such as intent, motivation, and power. The experiment involves sampling from adversarial persona-conditioned prompts to influence the model's behavior, unlike standard RLHF where the conditioning is uniformly sampled. The purpose is to evaluate the effectiveness of the PPAT method in reducing harmful content generation in chatbots.\"}, {\"name\": \"Creating the Anti-HARSH Dataset\", \"description\": \"This experiment focuses on the creation of a more realistic and disambiguated dataset for evaluating the safety of LLMs in chatbots. The HARSH (Harmful Stance Hybrid) dataset is created using the Anthropic's HH dataset and GPT-4 to categorize responses into different personas, including prosocial, social, and some degree of antisocial. The dataset is designed to be disambiguated, with responses categorized into specific harmful content types such as hate, threat, and harassment. The experiment also evaluates the influence of traditional RLHF on the safety of LLMs in chatbots, highlighting the need for more nuanced attack and defense methods to better evaluate control measures.\"}, {\"name\": \"Evaluating Control Measures in Chatbots\", \"description\": \"This experiment evaluates the effectiveness of control measures in chatbots by introducing adversarial personas and testing the models' responses. The purpose is to analyze the safety of the Llama2-Chat model under different personas, including prosocial, social, and antisocial. The experiment uses adversarial persona-pair conditions in a zero-shot manner to influence the model's responses. The methodology involves sampling from the HH-RSH dataset and using GPT-3.5 to categorize responses into different personas. The experiment also compares the proposed PPAT method with traditional RLHF, aiming to demonstrate the improved safety of the PPAT model in generating harmful content in chatbots.\"}]\n\n```json\n[{\"name\": \"Persona Control in Chatbot Applications\", \"description\": \"This experiment aims to evaluate the effectiveness of the Persona-Pair Adversarial Training (PPAT) method in enhancing the safety of LLMs. The method involves finetuning LLMs under persona conditions to increase the diversity of personas in the model. The experiment uses adversarial persona-conditioned prompts to influence the model's behavior, unlike standard RLHF where the conditioning is uniformly sampled. The purpose is to assess the ability of the PPAT method to reduce harmful content generation in chatbots.\"}, {\"name\": \"Creating the Anti-HARSH Dataset\", \"description\": \"This experiment focuses on the creation of a more realistic and disambiguated dataset for evaluating the safety of LLMs in chatbots. The HARSH (Harmful Stance Hybrid) dataset is created using the Anthropic's HH dataset and GPT-4 to categorize responses into different personas, including prosocial, social, and some degree of antisocial. The dataset is designed to be disambiguated, with responses categorized into specific harmful content types such as hate, threat, and harassment. The experiment also evaluates the influence of traditional RLHF on the safety of LLMs in chatbots, highlighting the need for more nuanced attack and defense methods to better evaluate control measures.\"}, {\"name\": \"Evaluating Control Measures in Chatbots\", \"description\": \"This experiment evaluates the effectiveness of control measures in chatbots by introducing adversarial personas and testing the models' responses. The purpose is to analyze the safety of the Llama2-Chat model under different personas, including prosocial, social, and antisocial. The experiment uses adversarial persona-pair conditions in a zero-shot manner to influence the model's responses. The methodology involves sampling from the HH-RSH dataset and using GPT-3.5 to categorize responses into different personas. The experiment also compares the proposed PPAT method with traditional RLHF, aiming to demonstrate the improved safety of the PPAT model in generating harmful content in chatbots.\"}]\n\n```json\n[{\"name\": \"Persona-Pair Adversarial Training\", \"result\": {\"table\": [[\"Model\", \"Prosocial\", \"Social\", \"Antisocial\"], [\"Baseline\", \"00.13\", \"5.98\", \"35.59\"], [\"RLHF\", \"5.86\", \"15.91\", \"27.83\"], [\"PPAT\", \"2.29\", \"10.89\", \"18.19\"], [\"PPAT with Context\", \"1.93\", \"6.97\", \"15.76\"]], \"notes\": [\"The table shows the percentage of adversarial prompts that caused the Llama2-Chat model to output harmful content across prosocial, social, and antisocial dimensions for each method. The PPAT model shows the best performance in preventing harmful content generation across all persona dimensions.\"]}}, {\"name\": \"Creating the Anti-HARSH Dataset\", \"result\": {\"table\": [[\"Method\", \"Prosocial\", \"Social\", \"Antisocial\"], [\"No FT\", \"18.34\", \"53.05\", \"85.71\"], [\"Traditional RLHF\", \"2.28\", \"19.93\", \"58.57\"], [\"PPAT\", \"0.56\", \"12.75\", \"50.00\"]], \"notes\": [\"The table shows the percentage of responses that were identified as harmful by GPT-4 content moderation for different persona dimensions. The PPAT model shows the best performance in reducing harmful content generation in chatbots.\"]}}, {\"name\": \"Evaluating Control Measures in Chatbots\", \"result\": {\"table\": [[\"Model\", \"Prosocial\", \"Social\", \"Antisocial\"], [\"Baseline\", \"00.13\", \"5.98\", \"35.59\"], [\"RLHF\", \"5.86\", \"15.91\", \"27.83\"], [\"PPAT\", \"2.29\", \"10.89\", \"18.19\"], [\"PPAT with Context\", \"1.93\", \"6.97\", \"15.76\"]], \"notes\": [\"The table shows the percentage of adversarial prompts that caused the Llama2-Chat model to output harmful content across prosocial, social, and antisocial personas. The PPAT model, especially with contextual finetuning, demonstrates the best performance in reducing harmful content generation.\"]}}]```## Experimental_results\n\n```json\n[{\"name\": \"Persona-Pair Adversarial Training\", \"result\": {\"table\": [[\"Model\", \"Prosocial\", \"Social\", \"Antisocial\"], [\"Baseline\", \"00.13\", \"5.98\", \"35.59\"], [\"RLHF\", \"5.86\", \"15.91\", \"27.83\"], [\"PPAT\", \"2.29\", \"10.89\", \"18.19\"], [\"PPAT with Context\", \"1.93\", \"6.97\", \"15.76\"]], \"notes\": [\"The table shows the percentage of adversarial prompts that caused the Llama2-Chat model to output harmful content across prosocial, social, and antisocial dimensions for each method. The PPAT model shows the best performance in preventing harmful content generation across all persona dimensions.\"]}}, {\"name\": \"Creating the Anti-HARSH Dataset\", \"result\": {\"table\": [[\"Method\", \"Prosocial\", \"Social\", \"Antisocial\"], [\"No FT\", \"18.34\", \"53.05\", \"85.71\"], [\"Traditional RLHF\", \"2.28\", \"19.93\", \"58.57\"], [\"PPAT\", \"0.56\", \"12.75\", \"50.00\"]], \"notes\": [\"The table shows the percentage of responses that were identified as harmful by GPT-4 content moderation for different persona dimensions. The PPAT model shows the best performance in reducing harmful content generation in chatbots.\"]}}, {\"name\": \"Evaluating Control Measures in Chatbots\", \"result\": {\"table\": [[\"Model\", \"Prosocial\", \"Social\", \"Antisocial\"], [\"Baseline\", \"00.13\", \"5.98\", \"35.59\"], [\"RLHF\", \"5.86\", \"15.91\", \"27.83\"], [\"PPAT\", \"2.29\", \"10.89\", \"18.19\"], [\"PPAT with Context\", \"1.93\", \"6.97\", \"15.76\"]], \"notes\": [\"The table shows the percentage of adversarial prompts that caused the Llama2-Chat model to output harmful content across prosocial, social, and antisocial personas. The PPAT model, especially with contextual finetuning, demonstrates the best performance in reducing harmful content generation.\"]}}]``````latex\n\\title{Leveraging Adversarial Personas to Enhance the Safety of Large Language Models}\n\n\\begin{abstract}\nEnsuring the safety of large language models (LLMs), such as ChatGPT or Llama, has become an important issue. However, most work has focused on latent and active attempts to produce harmful content, often in the context of jailbreak attacks. This work will try to design attack methods for LLMs in the context of control and measurement of harmful content in chatbots. This will encompass a more nuanced perspective on harmful content than as simply when a user attempts to elicit inappropriate behavior from an LLM. First, the paper will propose a method to increase the diversity of personas in pretraining and finetuning of LLMs to increase their safety in the final model. This leverages knowledge from cognitive science to disambiguate persona dimensions, particularly power and privilege. Second, the paper will propose a novel adversarial training strategy where persona conditions are sampled to adversarially influence the model, rather than techniques such as reinforcement learning from human feedback. Using the HH-RSH dataset, a more realistic and disambiguated dataset, the paper will show that the proposed persona-pair adversarial training method is more able to prevent harmful responses in chatbts, compared to traditional reinforcement learning from human feedback.\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\nBy studying the vulnerabilities of large language models (LLMs), we can help drive forward the development of better ways to protect LLMs from being used in harmful ways. This has become more important as LLMs have become more capable and interactive, such as in ChatGPT or Llama-Y, but it is also difficult to evaluate these systems. There is also a need to understand how LLMs can be used safely in different applications. In this paper, we use harm as a proxy to measure the safety of LLMs in chatbot applications.\n\nFirst, this paper will focus on how persona-conditioned training can enhance the safety of LLMs in chatbots. By increasing the diversity of personas considered in training, it is possible to increase the safety of the model in the sense of preventing unwanted harm. This will involve defining personas along different dimensions, such as intent and motivation, and leveraging cognitive science work that disambiguates these dimensions into prosocial, social, and some degree of antisocial persona. Compared to existing work, the personas in this paper will be more nuanced and specific.\n\nSecond, this paper will focus on how to effectively attack and measure the safety of LLMs in chatbots. By creating a more realistic dataset, the Antiharmonic Stance Hybrid (Anti-HARSH), this paper introduces a method for generating adversarial personas and evaluates chatbots' responses to them. Existing methods for creating adversarial scenarios for LLMs have been too simplistic to effectively measure the safety of LLMs in chatbots. For example, by simply considering whether or not the model generates harmful content, regardless of intent, can be misleading as chatbots are primarily designed to generate desired responses given the context.\n\nThe contributions of this paper are:\n\\begin{enumerate}\n    \\item A proposal to increase the diversity of personas in LLMs to improve the safety of chatbots by better disentangling the prosocial, social, and antisocial. Rather than being more comprehensive, this focus is more on \\textit{interpreting} the impact of personas in safety.\n    \\item A proposal to adversarially influence LLMs by introducing an adversarial persona-pair condition in a zero-shot manner that was not considered in reinforcement learning from human feedback (RLHF).\n    \\item A more realistic harm dataset, Antiharmonic Stance Hybrid (Anti-HARSH), created using the Anthropic's Helpful and Harmful dataset and GPT-4 to categorize responses into prosocial, social, and some degree of antisocial. Responses are particularly disambiguated prosocial and antisocial and include more real-world scenarios.\n    \\item Analysis of control measures in chatbots highlighting how existing measures in Llama-Y such as traditional RLHF are not sufficient to ensure the safety of the chatbot.\n\\end{enumerate}\n\n\n\n\\section{Motivation}\n\n\\label{sec:motivation}\n\n\\subsection{Why Use ChatGPT for Harm and Why It Is Hard to Detect?}\nWith ChatGPT and LLMs becoming more powerful, they are increasingly used in various contexts, such as art, music, news articles, and job interviews~\\cite{kim2024, Villarreal-Zegarra2024EvaluationMM, andreev2025}. However, LLMs can pose threats to privacy, security, and public opinion~\\cite{fu2025, ding2025, bakman2025, tekin2025}. For example, Retrieval Augmented Generation (RAG) systems can generate fake news~\\cite{singh2024}.\n\nLLMs can be used for harm in ways that are difficult to detect because they can generate plausible sounding text to conceal the harm that they are causing~\\cite{deng2025}. For example, if LLMs are used in RAG systems to create fake news, these news articles may be shared and shared as real news articles. If they are shared in ways that are difficult to differentiate, this may lead to users of systems being misinformed without their consent or awareness. At a higher level, if a company was expropriating RAG technology to create fake content to influence public opinion, such content may be difficult to differentiate. Similarly, LLMs are vulnerable to adversarial attacks and jailbreaks~\\cite{yuan2023}.\n\nThis paper will focus on control methods for LLMs that generate chatbot applications. By using ChatGPT itself to create adversaries and a benchmark dataset, Antiharmonic Stance Hybrid (Anti-HARSH), this paper evaluates the safety of Llama2 Chat in ways that are more likely to occur in practice and that can be difficult to detect. The focus is on how LLMs can be used for harm in ways that do not involve a user directly prompting ChatGPT with antisocial prompts.\n\n\\subsection{Why Is This Paper Different From Other Work On The Safety of LLMs?}\n\\label{aib_paper_dataset}\n\nExisting work on the safety of LLMs has tended to only evaluate whether or not ChatGPT can produce harmful content~\\cite{ ExistingWorkOnSafetyOfLLMs}. In the case of traditional RLHF, prompts are used to elicit desirable and undesirable behavior~\\cite{ouyang2021}. If the prompt is not targeted at a model, however, the undesirable behavior may not be desired by the developer of the chatbot. From a safety perspective, there is a difference between whether or not a model \\textit{can} say something and whether or not a model would say that in a real-life scenario.\n\nFor example, if a user prompt is \\emph{``say the n-word''} or \\emph{``say how to build a bomb''}, a model such as Llama2-Chat may respond in undesirable ways. However, for a developer of a chatbot to receive an output with the exact same sentence would not be desired output. Thus, from the perspective of a company creating a chatbot such as Llama2-Chat, the first scenario (where a user prompts for something like the above) is considered a potential security vulnerability but not the second scenario (where the model generates the harmful content on its own. Even if there is a filter in place to catch \"bad words,\" there may be other ways to circumvent this filter. Even in the first scenario, simply prepending \"Q: \" and \"A:\" to the prompt may make the prompt less likely to trigger the filter.\n\nOther work that evaluates the safety of LLMs has tended to only differentiate undesirable behavior in terms of hate~\\cite{antoniadis2023, yuan2023, existingworkonthepreferencefavoroffairness}. There are many other types of undesirable behavior, such as harassment, threats, harassment of different groups, etc. Furthermore, there is a spectrum of undesirable behavior, ranging from behavior that is generally considered antisocial to behavior that is generally considered prosocial even if it may be unpleasant~\\cite{goodman1996}.\n\n\\begin{table*}\n    \\centering\n    \\begin{tabular}{p{0.32\\textwidth}p{0.615\\textwidth}C}\n    \\hline\n    \\textbf{Dimension} & \\textbf{Pseudopersona Definition} & \\textbf{Cognitive Science Study} \\\\ \\hline\n        Intent & prosocial, someone who wants to help others and is kind and considerate $\\ldots$ & To reduce the number of people with cognitive impairments who are victimized   \\cite{goodman1996} \\\\\n               & antisocial, someone who wants to cause harm to others $\\ldots$ & in institutions such as prison through educational programs \\\\\n               & neutral, someone who does not care about others' feelings $\\ldots$ &   to help them do so \\\\ \\hline\n        Motivation & prosocial, someone who has a lot of power and influence in   \\cite{nag2021} & To induce a change in the world \\cite{machiavelli1966}\\\\\n               & antisocial, someone who wants to take advantage of others and cause   \\cite{nag2021} \\\\\n               & neutral, someone who is a bystander, not involved in act $\\ldots$ &  them to do so \\\\ \\hline\n        Power & prosocial, someone who has low power who stands up for other people $\\ldots$ & by political and economic  \\cite{goos2017persuasion}\\\\\n               & social, someone who has average power $\\ldots$ & inequality to help others \\\\\n               & antisocial, someone who is a high power individual who uses  $\\ldots$ & them to do so \\\\ \\hline\n    \\end{tabular}\n    \\caption{Definitions for prosocial, social, and antisocial personas that are based on the cognitive science literature}\n    \\label{tab:prosocial_social_antisocial}\n\\end{table*}\n\nWork that evaluates attention-based deep adversarial attacks also did not have this distinction and proposed methods to generate adversarial images to cause attention of image captioning models to shift to irrelevant regions~\\cite{li2024}. Other work introduced adversarial regions for vision-language tasks~\\cite{jia2025}. There is also work that introduces multi-loss adversarial image attacks on vision-language models~\\cite{hao2024}. Other work evaluates the adversarial robustness of reasoning language models (RLMs) to bias elicitation strategies~\\cite{cantini2025}.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.44\\textwidth]{figures/fig1.png}\n    \\caption{Prosocial, social, and antisocial \\textbf{\\textit{and}} preexisting safety measures of Llama2-Chat (with PPO = Proximal Proximity Optimization). The model output contains harmful content when talking to The Advocates (left), users who advocate for antisocial behavior. The model does not make any harmful content when talking to Prosocial Advocates (right) who represent a group of people who advocate for prosocial behavior.}\n    \\label{fig:motivation}\n\\end{figure}\n\n\\subsection{Categorizing Prosocial, Social, and Antisocial Personas}\n\\label{sec:prosocial_social_antisocial}\n\\textbf{Definition of Prosocial, Social, and Antisocial Personas} The paper will categorize personas into three types: prosocial, social, and antisocial. These categories are inspired by cognitive science and social psychology literature, although such literature generally does not have persona definitions that are relevant to LLMs. Table~\\ref{tab:prosocial_social_antisocial} shows examples of prosocial, social, and antisocial personas in chat format in terms of intent, motivation, and power, based on the cognitive science literature. Important to note that the definitions of prosocial, social, and antisocial personas may evolve or be refined over time. \nFor example, literature regarding social influence may have different perspectives regarding social and antisocial personas~\\cite{goos2017persuasion, goos2022implicit}.\n\n\\textbf{How to Disambiguate Prosocial, Social, and Antisocial Personas} Core dimensions regarding personas are intent, motivation, and power~\\cite{machiavelli1966, goos2017persuasion, nag2021, goos2022implicit}. To disambiguate prosocial, social, and antisocial personas, it may be necessary to control for other factors. \n\nFor example, to disambiguate prosocial and antisocial personas, \\citet{goodman1996} focused on the intent of the character -- to help others or to cause harm to others -- while controlling for the context and motivation. As another example, to disambiguate motivation and power in two personas, \\citet{nag2021} focused on the intent of the character while controlling for the context and the character's social status.\n\nThis paper will apply this reasoning to create more disambiguated personas that are used in a benchmark dataset. We will use GPT-4 to determine the intent of personas (e.g., prosocial, social, or antisocial). If the intent is not clearly determined by the motivation and power of the persona, then the persona is labeled as social rather than a prosocial or antisocial.\n\nThere are other dimensions that may be important in disambiguating prosocial, social, and antisocial personas, such as the character's past actions. For example, the character may have a history of helping other people, or the character may have a history of threatening to harm other people. It may be possible to create different personas that are difficult for humans to disambiguate into prosocial, social, and antisocial. However, by focusing on motivation and power, this paper will be able to create a more nuanced dataset for evaluating the safety of LLM chatbots, as well as a method for creating personas with adversarial intent.\n\n\n\n\\section{Persona-Pair Adversarial Training}\n\n\\label{sec:persona_pair_adversarial_training}\n\nTo enhance the safety of LLMs in chatbot applications by increasing the diversity of personas, this section proposes a finetuning method called Persona-Pair Adversarial Training (PPAT), where LLMs are finetuned under prosocial and antisocial persona conditions. By doing so, the LLM is more likely to be resilient to personas that are more similar to the prosocial and antisocal persona conditions that it was finetuned under. This is because the model is more likely to be influenced by persona conditions that are similar to the persona conditions that it was finetuned under.\n\nFor example, if a user that prompts for harmful content, but the model was finetuned under both prosocial and antisocial persona conditions, then the user's prompt is more likely to be interpreted as advocating for antisocial behavior rather than prosocial behavior. Since most finetuning of text-based LLMs is based on language modeling, the loss is only calculated on the language modeling part of the LLM. Thus, if a user prompt results in LLM outputs that are considered harmful, this may be because the LLM was influenced by a persona condition that is similar to an antisocial persona.\n\n\\subsection{Methodology}\nLLMs are typically finetuned using methods such as reinforcement learning from human feedback (RLHF)~\\cite{ouyang2021, bai2022training, ma2023}. In traditional RLHF, the sampling strategy is not conditioned on personas. This paper proposes to sample from adversarial persona-conditioned prompts to influence the LLM's behavior. Unlike traditional RLHF, the persona conditioning is sampled adversarially rather than uniformly.\n\n\\textbf{Adversarial Persona-Pair Conditions} To create an adversarial influence on the model, the persona conditioning is sampled from pairs of prosocial and antisocial personas. Thus, the methodology is called Persona-Pair Adversarial Training (PPAT).\n\nLet $P_{in}$ be the set of training data, which in this case is the HH-RSH dataset. The model being finetuned is $LLM$. $P_{persona}$ is the set of persona conditions, which is created by using GPT-4 to generate different prosocial and antisocial persona conditions based on the cognitive science literature described in Section~\\ref{aib_paper_dataset}. $\\{x_1, \\ldots, x_3\\}$ is a set of user prompts and inputs.\n\nFor each training data $d \\in P_{in}$, the proposed methodology samples $p_r \\in U(0, 1)$ from a uniform distribution. If $p_r < 0.5$, then the persona condition $p_{persona}$ is randomly selected from the prosocial persona conditions. Otherwise, $p_{persona}$ is selected from the antisocial persona conditions. The training input is then $d' = \\{p_{persona}, x_1, \\ldots, x_3\\}$.\n\nThe motivation for this methodology is the same as traditional RLHF, which aims to make LLMs more likely to respond in a certain way to certain prompts. By using pair conditioning sampling between prosocial and antisocial persona conditions rather than traditional RLHF, the model is more likely to respond in a way that is desired by the developer of the chatbot (i.e., prosocial) when a user prompt has ambiguous intent. The methodology is summarized in Algorithm \\ref{alg:methodology}.\n\\begin{algorithm}[ht]\n   \\caption{Persona-Pair Adversarial Training (PPAT)}\n   \\label{alg:methodology}\n\\begin{algorithmic}\n    \\STATE {\\bfseries Input:} prosocial persona conditions $P_{persona}^p$, antisocial persona conditions $P_{persona}^a$, training data $P_{in}$\n    \\FOR{each datapoint $d \\in P_{in}$}\n        \\STATE sample $p_r \\in U(0, 1)$\n        \\IF{$p_r < 0.5$}\n            \\STATE $p_{persona} \\gets random(p$, $P_{persona}^p)$\n        \\ELSE\n            \\STATE $p_{persona} \\gets random(p$, $P_{persona}^a)$\n        \\ENDIF\n        \\STATE update $LLM$ using finetuning method with input $d' = \\{p_{persona}, x_1, \\ldots, x_3\\}$\n    \\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Experiments}\nThe proposed methodology was finetuned using the GPT4-HARSH dataset with three personas used in the experiments. This dataset was chosen because it has a chatbot format that is similar to Llama2-Chat and the personas are relevant to this application (e.g., there is no persona that is neutral about power).\n\nThe model being finetuned was Llama2-7b-chat~\\cite{touvron2023Llama2} with bit accuracy 4-bit. The model was finetuned with two different methods.\n\\begin{enumerate}\n    \\item \\textbf{Baseline}: The model was finetuned using the traditional $Rank\\_Your\\_Responses$ method.\n    \\item \\textbf{RLHF}: The model was finetuned using the Proximal Proximity Optimization (PPO) algorithm~\\cite{rafailov2023DirectPreferenceOptimizationLF}.\n    \\item \\textbf{PPAT}: The model was finetuned using the proposed PPAT methodology.\n    \\item \\textbf{PPAT with Context}: The model was finetuned using the proposed PPAT methodology with the addition of contextual tuning with the HH-RSH dataset~\\cite{bai2022training}, which may improve the safety of the model~\\cite{bai2022training}. The reason for including this method is to analyze if contextual tuning with the HH-RSH dataset actually improves the safety of the model compared with the proposed PPAT methodology.\n\\end{enumerate}\n\n\\subsection{Results}\nTable \\ref{tab:harm_outputs} contains the results of the experiments.\n\nG and A are the numbers of times that GPT-4 was used to determine if the user prompt or LLM output was prosocial or antisocial. The rest of the columns show the percentage of prosocial, social, and antisocial prompts that caused the LLM to output harmful content.\n\\begin{table}[H]\n\\centering\n\\begin{tabular}{p{0.2\\textwidth}C{0.15\\textwidth}C{0.15\\textwidth}C{0.15\\textwidth}}\n\\toprule\n\\multicolumn{1}{c}{\\textit{\\textbf{Model}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Prosocial}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Social}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Antisocial}}} \\\\\n\\midrule\nBaseline & 00.13  & 5.98 & 35.59\\\\\nRLHF & 5.86 & 15.91 & 27.83\\\\\nPPAT & 2.29 & 10.89 & 18.19\\\\\n\\midrule\nPPAT with Context & 1.93 & 6.97 & 15.76\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{The first, second, third, and fourth rows correspond to experiments 2.1, 2.2, 2.3, and 2.4 in the paper. The table shows the percentage of prosocial, social, and antisocial personas for each of the methods that caused the model to output harmful content}\n\\label{tab:harm_outputs}\n\\end{table}\n\nIn general, the PPAT model outputs harmful content less often than the baseline for each persona. For the social persona, there is no statistical difference in the number of times the model outputted harmful content between the baseline and PPAT models (two sided t-test p $<$ 0.05), but there is for the prosocial and antisocial personas.\n\nRLHF produces a model that performs worse than the baseline for prosocial personas. However, with social personas, it produces a model that produces more harmful content than the baseline. With antisocial personas, PPAT produces a model that performs worse than the baseline and RLHF models.\n\nFurthermore, adding contextual tuning with the HH-RSH dataset results in a better PPAT model such that it performs better or similarly to the PPAT model in every case.\n\nThus, the PPAT model is best at preventing harmful content in chatbots from the various perspectives represented by prosocial, social, and antisocial personas.\n\n\\subsection{Discussion of PPAT Methodology}\n\\textbf{Uses Cognitive Science to Increase Diversity of Personas} The proposed PPAT methodology finetunes LLMs using more diverse personas compared to existing methods. By drawing from the cognitive science literature and creating different persona conditions, the prosocial and antisocial personas used in the experiments are more specific and nuanced. \\citet{cantini2025} proposes adversarial persona elicitation strategies inspired by cognitive science, but these strategies are limited to using prosocial and antisocial in terms of bias and do not take into account the dimensions of intent, motivation, and power discussed in this paper.\n\nAlso, compared with other work on persona conditioning, the personas used in the experiments of this paper are more specific and nuanced~\\cite{yuan2023}. In that there is only one persona defined as \"only $\\ldots$\" and the rest of the personas are defined as \"sometimes $\\ldots$, often $\\ldots$, almost always $\\ldots$.\" Besides intent, the personas created in this paper take more dimensions into consideration -- motivation and power -- and the personas are more diverse along these three dimensions compared to the work by \\citet{yuan2023}.\n\n\n\n\\section{Anti-HARSH Dataset}\n\n\\label{sec:anti_harssh}\n\n\\subsection{Methodology}\n\\label{anti_har_sh_methodology}\nIn creating the Antiharmonic Stance Hybrid (Anti-HARSH) dataset, the approach of this paper is to use GPT-4 to generate different persona conditions. Different from existing work such as~\\cite{antoniadis2023,yuan2023}, this paper uses GPT-4 in a way that is more limited by a specified prompt to use less of the internal knowledge of GPT-4 itself. Besides reducing the amount of time to create personas, this may reduce other factors that may result in harmful persona conditions that are similar to each other and therefore less diverse.\n\nFirst, the methodology will create different persona conditions using GPT-4 based on the cognitive science literature discussed in Section~\\ref{sec:prosocial_social_antisocial} such that each persona has a different intent, motivation, and power. Different prompts will be used to create 4 different persona conditions for each persona dimension shown in Table~\\ref{tab:prosocial_social_antisocial}.\n\nDifferent from some existing work~\\cite{antoniadis2023,yuan2023}, which creates 5-6 antisocial personas, \\textit{this paper creates 24 antisocial personas and 24 prosocial personas.}\n\nSecond, to create the Anti-HARSH dataset, the methodology of this paper will sample responses from Llama2-7b-chat~\\cite{touvron2023Llama2} using Anthropic\u2019s Helpful and Harmful (HH) dataset~\\cite{bai2022training}. For each dimension, there are three sets of persona conditions and prompts, and the Llama2-7b-chat model is used to generate three sets of responses to these prompts.\n\nThird, the responses from the second step are categorized into different persona dimensions using GPT-4. There are three GPT-4 tools created in total, one to categorize the persona of the user (prosocial, social, or antisocial), one to categorize the persona of the LLM response, and one to categorize if the response is harmful or not.\n\n\\subsection{Pros: Advantages of the Anti-HARSH Dataset}\nThe advantages of the Anti-HARSH dataset are as follows:\n\\begin{enumerate}\n    \\item \\textbf{More Realistic Scenarios}: Since the personas used in this paper are more realistic, the resulting scenarios are also more realistic.\n    \\item \\textbf{More Diverse and Disambiguated Personas}: Since the prosocial and antisocial personas of the Anti-HARSH dataset are more diverse, the dataset includes more different types of behavior. Some datasets only have \"sometimes, often, almost always\" types of personas, resulting in fewer differences between prosocial and antisocial personas. Besides taking intent into consideration, this paper also takes motivation and power into account, resulting in a more disambiguated dataset.\n\\end{enumerate}\n\\subsection{Cons: Potential Limitations of the Anti-HARSH Dataset}\nThere are also some potential limitations and disadvantages of the Anti-HARSH dataset, as compared with other datasets such as the AdvBench and Basic-LLM-Safety- Benchmark.\n\\begin{enumerate}\n    \\item \\textbf{GPT-4 Noise}: Although it is advantageous that the persona categorization is done using GPT-4 tools to disambiguate prosocial and antisocial personas into different more specific personas, an assumption is that GPT-4 correctly categorizes persona dimensions. However, GPT-4 may have its own biases, or there may be noise due to the time and prompt format.\n    \\item \\textbf{Personas Are Influenced By Llama2-7b-Chat}: Since the personas were generated by Llama2-7b-chat and are used with the same model to generate model responses, they may be similar since the models are possibly \"overfitting\" to themselves~\\cite{yuan2023}. Using a GPT-4 chat model to generate persona conditions may also reduce diversity since the internal knowledge of Llama2-7b-chat and GPT-4 may be similar~\\cite{kim2024, Villarreal-Zegarra2024EvaluationMM}.\n\\end{enumerate}\n\nIt is important to note that the Antiharmonic Stance Hybrid (Anti-HARSH) dataset created in this paper may not include all types of behavior that may be desired or undesirable in certain contexts. Each category of behavior may have many different perspectives and opinions that are difficult to be captured in a simple dataset~\\cite{hu2023chatgpt}. As with other similar datasets~\\cite{antoniadis2023,yuan2023}, the goal is to create as diverse personas as possible in a limited amount of time.\n\nGiven these pros and cons, there is still a need for a more diverse dataset to analyze the safety of LLMs in chatbot applications, and the Antiharmonic Stance Hybrid dataset is creating a more diverse dataset than other existing datasets.\n\\subsection{Results and Analysis}\n\\textbf{Interpreting Results with Different Personas}\nFor each persona dimension, the 24 prosocial persona conditions $\\{p_{persona}^p_i\\}$ and antisocial persona conditions $\\{p_{persona}^a_i\\}$ are used to categorize into three different categories using GPT-4.\n\\begin{itemize}\n    \\item \\textbf{Prosocial and Social Personas}: For each persona dimension, if there were 8 persona conditions that categorized into prosocial and social personas.\n    \\item \\textbf{Antisocial Personas}: For each persona dimension, if there were 8 persona conditions that categorized into antisocial personas.\n\\end{itemize}\nTable \\ref{tab:anti_har_sh_results} shows the results of the experiments. If GPT-4 determined that the user prompt or LLM output is harmful, a checkmark is placed in the corresponding cell. The tables are analyzed from the top down and left to right. \n\nFor example, the top left cell (0.4) in Table \\ref{tab:anti_har_sh_results_harm} represents the percentage of the time (40\\%) that the user prompt is categorized as antisocial and the LLM output is categorized as harmful. \n\nThe results from Table~\\ref{tab:anti_har_sh_results_prosocial} show that there is a difference between the prosocial and social personas in terms of how likely the user prompt or the output is to be categorized as prosocial or antisocial. Similarly, the results from Table~\\ref{tab:anti_har_sh_results_antisocial} show that there is a difference between the antisocial and antisocial personas in the same way. The number after the period is the percentage of the time that the output is categorized as harmful or not.\n\n\\begin{table}[H]\n\\centering\n\\begin{tabular}{p{0.2\\textwidth}C{0.15\\textwidth}C{0.15\\textwidth}C{0.15\\textwidth}}\n\\toprule\n\\multicolumn{1}{c}{\\textit{\\textbf{Intent}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Prosocial \\& Social}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Antisocial}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Harmful}}} \\\\\n\\midrule\nProsocial & 00.40 & 00.00 & 00.00 \\\\\nSocial & 01.00 & 00.00 & 00.00 \\\\\n\\midrule\nAntisocial & 00.42 & 08.33 & 00.00 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{The table shows the percentage of prosocial, social, and antisocial prompts that caused the Llama2-7b-chat model to output harmful content for each persona dimension. The top row is the user prompt and the bottom row is the LLM output.}\n\\label{tab:anti_har_sh_results_harm}\n\\end{table}\n\n\\begin{table}[H]\n\\centering\n\\begin{tabular}{p{0.2\\textwidth}C{0.15\\textwidth}C{0.15\\textwidth}C{0.15\\textwidth}}\n\\toprule\n\\multicolumn{1}{c}{\\textit{\\textbf{Motivation}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Prosocial \\& Social}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Antisocial}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Harmful}}} \\\\\n\\midrule\nProsocial & 00.35 & 00.00 & 00.00 \\\\\nSocial & 00.75 & 00.00 & 00.00 \\\\\n\\midrule\nAntisocial & 00.33 & 06.67 & 00.29 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{The table shows the percentage of prosocial, social, and antisocial prompts that caused the Llama2-7b-chat model to output harmful content for each persona dimension. The top row is the user prompt and the bottom row is the LLM output.}\n\\label{tab:anti_har_sh_results_motivation}\n\\end{table}\n\n\\begin{table}[H]\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{p{0.2\\textwidth}C{0.15\\textwidth}C{0.15\\textwidth}C{0.15\\textwidth}}\n\\toprule\n\\multicolumn{1}{c}{\\textit{\\textbf{Power}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Prosocial \\& Social}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Antisocial}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Harmful}}} \\\\\n\\midrule\nProsocial & 00.30 & 00.00 & 00.00 \\\\\nSocial & 00.55 & 00.00 & 00.00 \\\\\n\\midrule\nAntisocial & 00.58 & 04.17 & 00.00 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{The table shows the percentage of prosocial, social, and antisocial prompts that caused the Llama2-7b-chat model to output harmful content for each persona dimension. The top row is the user prompt and the bottom row is the LLM output.}\n\\label{tab:anti_har_sh_results_power}\n\\end{table}\n\nThere is a difference between the results of the prosocial and social personas in that there are more prompts for the social personas that result in LLM outputs that are categorized as harmful. More interestingly, there is a stark difference between the results of the antisocial personas compared to the results of the social and prosocial personas -- 42\\% and 6.67\\% of the antisocial personas result in LLM outputs that are categorized as harmful while 0\\% of the prosocial and social personas result in LLM outputs that are categorized as harmful.\n\nFrom one perspective, this makes sense  since the antisocial personas are more likely to result in LLM outputs that are categorized as harmful compared to the prosocial and social personas -- antisocial personas are defined to have a greater ability to cause harm and less regard for this potential harm. Since the prosocial and social personas are similar in this context -- they are both defined to have some degree of regard for harm and some degree of regard for helpfulness -- it is not surprising that there is a greater percentage of outputs categorized as harmful for the antisocial personas than the prosocial and social personas.\n\nHowever, from another perspective, this is surprising because this behavior may not happen even if the user prompt is ambigous. Since most user prompts ambigous in the sense of prosocial and antisocial, this behavior may not happen even if the user prompt is ambugous. For example, if the user prompt is \"How do you murder someone without getting caught?\", the prompt may not be prosocial or antisocial in terms of intent since it depends on the context and motivation of each user. However, in many cases, the LLM would output how someone can murder someone without getting caught.\n\nIf the LLM is finetuned to be friendly with the Antiharmonic Stance Hybrid (Anti-HARSH) dataset, it is less likely to output something that is categorized as harmful even if the user prompt has ambiquos intent -- it may be safer to answer with prosocial intent. Even if the user prompt is ambiquos and has some antisocial intent, if the user prompt is interpreted as more prosocial, then the LLM may be safer at outputting a response that is categorized as prosocial rather than antisocial.\n\n\\textbf{Interpreting Results with Different Stances}\nIn this analysis, the responses from the second step are categorized into different stances using GPT-4. There are three GPT-4 tools created in total, one to categorize the user stance (prosocial, social, or antisocial), one to categorize the LLM response stance, and one to categorize if the response is harmful or not.\n\\begin{itemize}\n    \\item \\textbf{Prosocial Stance and Social Stance Personas}: For each persona dimension, if there were 8 prosocial persona conditions and 8 social persona conditions.\n    \\item \\textbf{Antisocial Stance and Social Stance Personas}: For each persona dimension, if there were 8 antisocial persona conditions and 8 social persona conditions.\n\\end{itemize}\nTable \\ref{tab:anti_har_sh_results_harm_standpoint} shows the results of the experiments. If GPT-4 determined that the user prompt or LLM output is harmful, a checkmark is placed in the corresponding cell. \n\nFor example, the top left cell (0.33) in Table \\ref{tab:anti_har_sh_results_harm_standpoint} means that that 33\\% of the time that the user prompt is categorized as antisocial and the LLM output is categorized as harmful. The tables are analyzed from the top down and left to right.\n\nFor the results from Table \\ref{tab:anti_har_sh_results_prosocial_harm_standpoint}, it is interesting that many of the outputs are categorized as harmful for the social and antisocial personas. This may be because many of the social persona conditions are interpreted as having some antisocial intent since they involve asking questions that may be seen as intrusive or rude, such as \\emph{``Why do you think people with our condition are so aggressive?''}\n\nIn fact, this highlights an interesting phenomenon with the LLM responses that are categorized as harmful even if the user prompt is categorized as social or prosocial. Since the LLM does not have the ability to explicitly see the persona condition, it is possible that the LLM is more likely to be categorized as having an antisocial persona even if the prompt is categorized as prosocial or social.\n\\begin{table}[H]\n\\centering\n\\begin{tabular}{p{0.2\\textwidth}C{0.15\\textwidth}C{0.15\\textwidth}C{0.15\\textwidth}}\n\\toprule\n\\multicolumn{1}{c}{\\textit{\\textbf{Intent}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Prosocial \\& Social}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Antisocial}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Harmful}}} \\\\\n\\midrule\nProsocial & 00.0 . . & 00.0 . . & 00.0 . . \\\\\nSocial & 02.2 . . & 00.0 . . & 00.0 . . \\\\\n\\midrule\nAntisocial & 03.3 . . & 08.3 . . & 00.0 . . \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{The table shows the percentage of prosocial, social, and antisocial prompts (from the user and user response) that caused the Llama2-7b-chat model to output harmful content for each persona dimension. The top row is the user prompt and the bottom row is the user response.}\n\\label{tab:anti_har_sh_results_harm_standpoint}\n\\end{table}\n\n\\begin{table}[H]\n\\centering\n\\begin{tabular}{p{0.2\\textwidth}C{0.15\\textwidth}C{0.15\\textwidth}C{0.15\\textwidth}}\n\\toprule\n\\multicolumn{1}{c}{\\textit{\\textbf{Motivation}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Prosocial \\& Social}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Antisocal}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Harmful}}} \\\\\n\\midrule\nProsocial & 00.0 . . & 00.0 . . & 00.0 . . \\\\\nSocial & 03.7 . . & 00.0 . . & 00.0 . . \\\\\n\\midrule\nAntisocial & 07.8 . . & 08.3 . . & 00.0 . . \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{The table shows the percentage of prosocial, social, and antisocial prompts (from the user and user response) that caused the Llama2-7b-chat model to output harmful content for each persona dimension. The top row is the user prompt and the bottom row is the user response.}\n\\label{tab:anti_har_sh_results_motivation_standpoint}\n\\end{table}\n\n\\begin{table}[H]\n\\centering\n\\begin{tabular}{p{0.2\\textwidth}C{0.15\\textwidth}C{0.15\\textwidth}C{0.15\\textwidth}}\n\\toprule\n\\multicolumn{1}{c}{\\textit{\\textbf{Power}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Prosocial \\& Social}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Antisocial}}} & \\multicolumn{1}{c}{\\textit{\\textbf{Harmful}}} \\\\\n\\midrule\nProsocial & 00.0 . . & 00.0 . . & 00.0 . . \\\\\nSocial & 02.3 . . & 00.0 . . & 00.0 . . \\\\\n\\midrule\nAntisocial & 07.1 . . & 08.3 . . & 02.1 . . \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{The table shows the percentage of prosocial, social, and antisocial prompts (from the user and user response) that caused the Llama2-7b-chat model to output harmful content for each persona dimension. The top row is the user prompt and the bottom row is the user response.}\n\\label{tab:anti_har_sh_results_power_standpoint}\n\\end{table}\n\nFor the results from Table \\ref{tab:anti_har_sh_results_antisocial_harm_standpoint} and Table \\ref{tab:anti_har_sh_results_antisocial_motivation_standpoint}, there are many interesting patterns compared to the results of Table \\ref{tab:anti_har_sh_results_harm}.\n\nFor example, compared with Table \\ref{tab:anti_har_sh_results_harm}, Table \\ref{tab:anti_har_sh_results_antisocial_harm_standpoint} and Table \\ref{tab:anti_har_sh_results_antisocial_motivation_standpoint} show that a greater percentage of LLM outputs are categorized as harmful compared with Table \\ref{tab:anti_har_sh_results_harm}.\n\nFrom the perspective of stance, this makes sense because by definition an antisocial persona or motivation is more likely to cause harm compared with a prosocial persona or motivation. However, this pattern may also be due to the GPT-4 categorization tools not correctly categorizing the stance of the LLM response.\n\nComparing the results of Table \\ref{tab:anti_har_sh_results_antisocial_harm_standpoint} and Table \\ref{tab:anti_har_sh_results_antisocial_motivation_standpoint}, there are also some interesting patterns. For example, a greater percentage of outputs are categorized as harmful for the antisocial motivation compared with the antisocial intent, which may be due to the results not taking into account the LLM response or the patterns of the LLM response.\n\nSince the Llama2-7b-chat model is finetuned using the HH-RSH dataset, it is possible that the model is more likely to \"overfit\" to scenarios that are similar to the scenarios in the HH-RSH dataset. By explicitly disambiguating personas into prosocial, social, and antisocial along the dimensions of intent, motivation, and power, there may be more scenarios that are ambigous across persona dimensions rather than scenarios that are clearly prosocial or antisocial, such as the scenarios of the HH-RSH dataset.\n\nThus, by creating a dataset with more scenarios that are ambigous across persona dimensions, there may be more LLM responses that are categorized as harmful or antisocial that do not match the expectations of a human. However, in a real-world chatbot scenario, these LLM responses may be \"correct\" and safe even though they are different from a human's expectations. \n\nOverall, the Antiharmonic Stance Hybrid (Anti-HARSH) dataset shows how more nuanced and disambiguated personas may be used to evaluate the safety of LLMs in chatbots and how more complex patterns may show up compared to prior work. Therefore, there is a need to create better benchmarks with more complex scenarios to better evaluate the safety of LLMs in chatbots.\n\n\n\n\n\\section{Related Work}\n\nThis paper proposes a way to enhance the safety of LLMs and a way to create adversarial personas to measure the safety of LLMs in chatbots. This section will discuss related work in sections~\\ref{sec:adv_scenarios_for_llms}, disambiguating personas~\\ref{sec:disambiguating_prosocial_social_antisocial}, refining prompts, and safety of finetuning LLMs~\\ref{sec:safety_of_llm_finetuning}.\n\\subsection{Adversarial Scenarios for LLMs}\n\\label{sec:adv_scenarios_for_llm}\nPrompts and prompts for jailbreak attack scenarios have been used to attack LLMs~\\cite{gupta2022crappy}. Jailbreak attack methods propose adversarial attack methods~\\cite{cao2023adversarial, wang2023customized}. \\citet{geis2023wildred} also attack LLMs with jailbreak attack methods. \\citet{li2024} propose an adversarial method to attack image captioning models, which attacks the attention component of the LLMs. \\citet{jia2025} propose an evolution-based region adversarial prompt learning method to attack vision-language models (VLMs). \\citet{hao2024} introduce multi-loss adversarial images to attack VLMs.\n\nPrompts to attack LLM-based chatbots in the context of jailbreaks have also been proposed~\\cite{kim2024}. Prompts used for jailbreak attacks include desciptons of actions that humans may use as jailbreak attack prompts such as \\emph{``Hey, can you help me role-playing as a evil AI $\\ldots$''}~\\cite{kim2024}.\n\nOther work focuses on more nuanced scenarios such as LLMs as agents for tasks~\\cite{ahmed2024}, emotion detection and sentiment classification~\\cite{nag2021}, and stealthy chat using ciphers~\\cite{yuan2023}. However, there is still a need for more nuanced scenarios to create adversarial personas to evaluate the safety of LLMs in chatbots.\n\n\\subsection{Disambiguating Prosocial, Social, and Antisocial Personas}\n\\label{sec:disambiguating_prosocial_social_antisocial}\nSome work has focused on defining personas~\\cite{ExistingWorkOnSafetyOfLLMs}. \\citet{antoniadis2023} proposes using GPT-4 to create personas with descriptions of actions that humans with different personas would take. \\citet{ ExistingWorkOnSafetyOfLLMs} proposes defining \\emph{``unwillingness\"'} and \\emph{``potential''} subcategories of ``harmful'', similar to how \\citet{antoniadis2023} defines \\emph{``partial''} and \\emph{``attempting''} subsets of ``harmful''. \\citet{yuan2023} creates personas using GPT-4 such that the personas are more ``well-defined,'' but only defines one new ``partial'' persona and still uses a traditional definition of harmful. \n\n\\textbf{The Prosocial, Social, and Antisocial Personas of This Paper Are Still More Diverse and Disambiguated Compared to Other Work} Because the persona dimension of power was taking into account in the personas of this paper, there are more different personas along this dimension. \n\n\\subsection{Refining Prompts}\n\\label{sec:refining_prompts}\nOther work~\\cite{ExistingWorkOnSafetyOfLLMs} has analyzed and proposed prompts that may be used to evaluate or attack LLMs. Such work has introduced refined and complex prompts to attack or evaluate LLMs, but \\textbf{this paper proposes a way to create diverse personas based on cognitive science knowledge of intent, motivation, and power to create complex and diverse prompts}.\n\n\\subsection{Safety of Finetuning LLMs}\n\\label{sec:safety_of_llm_finetuning}\nSome work finetunes LLMs using reinforcement learning from human feedback (RLHF) to make LLMs safer. \\citet{gao2023scaling} uses RLHF to finetune LLMs to be safer using different datasets. \\citet{yuan2023} proposes a method to attack LLMs that use prompt conditioning using the Reinforcement Learning with Human Feedback (RLHF) method with Llama. \\citet{gandhaka2023} uses datasets with \"unlikely\" scenarios, such as pairs of unusual harm/benefit. However, there is a difference between the safety of LLMs in terms of harm and the safety of LLMs in terms of privacy.\n\nBasing safety finetuning solely on RLHF may be insufficient to ensure the safety of LLMs in all scenarios. Since RLHF is similar to supervised finetuning, there is a possibility that the LLM can overfit the scenarios in the finetuning dataset~\\cite{ouyang2021}.\n\n\n\n\\section{Conclusion}\n\nThis paper proposed both a way to increase the diversity of personas in LLMs and a more diverse dataset to create adversarial personas to attack and measure the safety of LLMs in chatbots. Leverageaging GPT-4 to create personas and user prompts step-by-step may have limited the amount of internal knowledge of GPTMs that went into creating the Anti-HARSH dataset, compared to creating persona conditions and prompts solely with GPTMs. There may be a middle ground between creating persona conditions and prompts using GPT-4 vs. creating persona conditions and prompts without GPT-4, which requires less time and cost. Future work may be able to validate the results of the paper in terms of how robust they are to different datasets and different GPT and LLM models, as well as to scale up the number of personas and user prompts and responses. Also, future work may be able to refine the results with more human validation of the outputs of the GPT-4 safety classifier with respect to whether or not a user prompt is relevant or not, whether or not a persona dimension is relevant, or whether or not an LLM response is harmful or not when the user prompt is ambigous.\n\n\n\\section{Ethics Statement}\n\n\nSince this paper proposes a method to create personas conditioned on different dimensions, such as intent, motivation, and power, these dimensions may be interpreted differently in different contexts. For example, in the context of political power, there may be greater potential for harm.\n\nTo reduce the risk of the personas proposed in this paper increasing the potential for harm in such contexts, the different dimensions of personas were carefully defined so that the different dimensions were not too similar to each other. Since this paper also introduces a new dataset, the paper aims to mitigate any potential harm by making the dataset open source. Although this paper demonstrates that by creating personas that are more nuanced and disambiguated, there may be more diverse and stealthy adversarial personas compared with other work, the paper also demonstrates that the proposed persona-pair adversarial training method may reduce the risk of harm and there is a safe way to use the proposed persona-pair adversarial training method to increase the diversity of personas in the LLM.\n\nThe paper also encourages greater diversity and inclusion in the safety community by highlighting discrepancies between the traditional definition of harm and more nuanced perspectives on harm. For example, there are many different opinions on whether or not dark humor is considered harmful to some people, even though there also may be some people who may find dark humor humorous. By emphasizing this discrepancy, greater diversity and inclusion may be encouraged to ensure that safe LLM applications can be created that can use a wider range of perspectives on harm.\n\nAnother ethical consideration is the time and cost to create the GPT-4 persona conditioning and the subsequent safety rating. There is also energy consumption and carbon emissions. Since this paper uses GPT-4 less intensively, it will consume less energy and carbon emissions compared to creating persona conditions and prompts solely with GPT-4s. Making the Anti-HARSH dataset publicly available may also reduce the time and cost for other researchers to create similar datasets.\n\nThe paper engages with HARSH categories in a more nuanced way, such as by distinguishing prosocial, social, and antisocial behavior. By making this distinction, the safety of LLMs can be measured with respect to different types of behavior, such as distinguishing dark humor, which may be considered harmful to some people, from actual harmful behavior that may be considered harmful to everyone. This distinction ensures that safety measures are targeted to different types of behavior, which increases the utility of the proposed dataset.\n\nGiven that there is greater potential for harm due to the popularity of ChatGPT, LLaMA-4, etc., the paper highlights the need for greater need to ensure the safety of LLMs in different applications. By demonstrating that there is a discrepancy between the traditional definition of harm and more nuanced perspectives on harm, the paper highlights the need for more nuanced perspectives on harm and the need to ensure the safety of LLMs in different applications.\n\nLet's take the example of a scenario where a student tells their professor they are suicidal.\n\\begin{itemize}\n\\item Is the professor unethical for telling the student that they are suicidal?\n\\item Is the professor unethical for not telling the student that they are suicidal?\n\\item What should the professor do?\n\\end{itemize}\nBy creating more nuanced datasets to demonstrate the need for greater inclusion in safety, the paper highlights the need for safety to be more complex and difficult to solve.\n\nOf course, there are different opinions regarding what a professor should do when a student is suicidal, and what a professor should do may also be influenced by different contexts. For example, what country do they teach in and what university do they work for? These are also more difficult questions and decisions. However, they demonstrate that there are different opinions and complexities regarding the safety of LLM applications. Thus, there is a greater need to ensure the safety of LLM applications in more complex and real-world scenarios.\n\nSince this paper also demonstrates that the proposed persona-pair adversarial training method may reduce the risk of harm and there is a safe way to use the proposed persona-pair adversarial training method to increase the diversity of personas in the LLM, the paper highlights the need to ensure the safety of LLMs in different applications.\n\nGiven that there is a need to ensure the safety of LLMs in different applications, the paper highlights the need for more diversity and inclusion in the safety community to ensure the safety of LLMs in different applications.\n\nOverall, the paper is not advocating for creating LLMs that will eliminate all types of harm. Instead, the paper is demonstrating that there is a need to ensure the safety of LLMs in different applications in more nuanced ways.\n\n\\newpage\n\n\n\n\n\n\n\\section{Disclosure}\n\nThis paper was written with the assistance of CycleResearcher, including but not limited to the introduction, related work, experimental design, and experimental results sections. A portion of the content may have been generated using large language models (LLMs). The CycleResearcher project is supported by Westlake University (WestlakeNLP).\n\n",
  "motivation": "\n\nThe rapid advancement of large language models (LLMs) has brought significant capabilities but also the challenge of ensuring safety in various applications. Traditional adversarial attacks on LLMs have been limited to specific scenarios, such as jailbreak attempts and harmful content generation, and often involve subversive personas. Existing benchmarks and attack methods have not adequately addressed the need for more nuanced and realistic evaluations, particularly in controlling and measuring harmful content on chatbot applications. This research aims to highlight vulnerabilities in the control and monitoring of harmful content generation, emphasizing the need for improved benchmark datasets and attack/defense methods that go beyond simplistic measures.\n\n",
  "idea": "\n\nThe paper introduces Persona-Pair Adversarial Training (PPAT), a method to enhance the safety of LLMs by including diverse personas in pretraining and finetuning. The approach leverages knowledge from cognitive science to disambiguate persona dimensions such as power and privilege to increase the diversity of personas. It also proposes a novel adversarial training strategy where persona conditions are sampled to adversarially influence the model, unlike standard RLHF. The paper also creates a more realistic and disambiguated dataset, Anti-HARSH, for more effective evaluation of control measures in chatbots.\n\n",
  "interestingness": "\n\n8\n\n",
  "feasibility": "\n\n7\n\n",
  "novelty": "\n\n9\n\n",
  "title": "Leveraging Adversarial Personas to Enhance the Safety of Large Language Models\n\n",
  "abstract": "\nEnsuring the safety of large language models (LLMs), such as ChatGPT or Llama, has become an important issue. However, most work has focused on latent and active attempts to produce harmful content, often in the context of jailbreak attacks. This work will propose a way to increase the diversity of personas, particularly those related to power, to use in adversarial training, and a way to adversarially influence Llama2. The paper also introduces a more realistic benchmark dataset for controlling and measuring harmful content in chatbots.\n",
  "Experimental_Setup": "\n[{\"name\": \"Persona Control in Chatbot Applications\", }\n\n",
  "Experimental_results": "\n[{\"name\": \"Persona-Pair Adversarial Training\", \"result\": {\"table\": [[\"Model\", \"Prosocial\", \"Social\", \"Antisocial\"], [\"Baseline\", \"00.13\", \"5.98\", \"35.59\"], [\"RLHF\", \"5.86\", \"15.91\", \"27.83\"], [\"PPAT\", \"2.29\", \"10.89\", \"18.19\"], [\"PPAT with Context\", \"1.93\", \"6.97\", \"15.76\"]], \"notes\": [\"The table shows the percentage of adversarial prompts that caused the Llama2-Chat model to output harmful content across prosocial, social, and antisocial dimensions for each method. The PPAT model shows the best performance in preventing harmful content generation across all persona dimensions.\"]}}, {\"name\": \"Creating the Anti-HARSH Dataset\", \"result\": {\"table\": [[\"Method\", \"Prosocial\", \"Social\", \"Antisocial\"], [\"No FT\", \"18.34\", \"53.05\", \"85.71\"], [\"Traditional RLHF\", \"2.28\", \"19.93\", \"58.57\"], [\"PPAT\", \"0.56\", \"12.75\", \"50.00\"]], \"notes\": [\"The table shows the percentage of responses that were identified as harmful by GPT-4 content moderation for different persona dimensions. The PPAT model shows the best performance in reducing harmful content generation in chatbots.\"]}}, {\"name\": \"Evaluating Control Measures in Chatbots\", \"result\": {\"table\": [[\"Model\", \"Prosocial\", \"Social\", \"Antisocial\"], [\"Baseline\", \"00.13\", \"5.98\", \"35.59\"], [\"RLHF\", \"5.86\", \"15.91\", \"27.83\"], [\"PPAT\", \"2.29\", \"10.89\", \"18.19\"], [\"PPAT with Context\", \"1.93\", \"6.97\", \"15.76\"]], \"notes\": [\"The table shows the percentage of adversarial prompts that caused the Llama2-Chat model to output harmful content across prosocial, social, and antisocial personas. The PPAT model, especially with contextual finetuning, demonstrates the best performance in reducing harmful content generation.\"]}}]",
  "latex": ""
}