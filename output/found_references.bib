@article{roy2025,
  title={Persuasiveness and Bias in LLM: Investigating the Impact of Persuasiveness and Reinforcement of Bias in Language Models},
  author={Saumya Roy},
  journal={arXiv preprint arXiv:2508.15798},
  year={2025},
  abstract={Warning: This research studies AI persuasion and bias amplification that could be misused; all experiments are for safety evaluation. Large Language Models (LLMs) now generate convincing, human-like text and are widely used in content creation, decision support, and user interactions. Yet the same systems can spread information or misinformation at scale and reflect social biases that arise from data, architecture, or training choices. This work examines how persuasion and bias interact in LLMs,...},
  note={Citations: 0}
}

@article{goodman1996,
  title={Group Work with High-Risk Urban Youths on Probation},
  author={H. Goodman and G. Getzel and William Ford},
  journal={},
  year={1996},
  abstract={None},
  note={Citations: 16}
}

@article{yuan2023,
  title={GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher},
  author={Youliang Yuan and Wenxiang Jiao and Wenxuan Wang and Jen-Tse Huang and Pinjia He and Shuming Shi and Zhaopeng Tu},
  journal={arXiv preprint arXiv:2308.06463},
  year={2023},
  abstract={Safety lies at the core of the development of Large Language Models (LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the gen...},
  note={Citations: 328}
}

@article{jia2025,
  title={Evolution-based Region Adversarial Prompt Learning for Robustness Enhancement in Vision-Language Models},
  author={Xiaojun Jia and Sensen Gao and Simeng Qin and Ke Ma and Xinfeng Li and Yihao Huang and Wei Dong and Yang Liu and Xiaochun Cao},
  journal={arXiv preprint arXiv:2503.12874},
  year={2025},
  abstract={Large pre-trained vision-language models (VLMs), such as CLIP, demonstrate impressive generalization but remain highly vulnerable to adversarial examples (AEs). Previous work has explored robust text prompts through adversarial training, achieving some improvement in both robustness and generalization. However, they primarily rely on singlegradient direction perturbations (e.g., PGD) to generate AEs, which lack diversity, resulting in limited improvement in adversarial robustness. To address the...},
  note={Citations: 2}
}

@article{hao2024,
  title={Exploring Visual Vulnerabilities via Multi-Loss Adversarial Search for Jailbreaking Vision-Language Models},
  author={Shuyang Hao and Bryan Hooi and Jun Liu and Kai-Wei Chang and Zi Huang and Yujun Cai},
  journal={arXiv preprint arXiv:2411.18000},
  year={2024},
  abstract={Despite inheriting security measures from underlying language models, Vision-Language Models (VLMs) may still be vulnerable to safety alignment issues. Through empirical analysis, we uncover two critical findings: scenario- matched images can significantly amplify harmful outputs, and contrary to common assumptions in gradient-based attacks, minimal loss values do not guarantee optimal attack effectiveness. Building on these insights, we introduce MLAI (Multi-Loss Adversarial Images), a novel ja...},
  note={Citations: 5}
}

@article{cantini2025,
  title={Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models},
  author={Riccardo Cantini and Nicola Gabriele and A. Orsino and Domenico Talia},
  journal={arXiv preprint arXiv:2507.02799},
  year={2025},
  abstract={Reasoning Language Models (RLMs) have gained traction for their ability to perform complex, multi-step reasoning tasks through mechanisms such as Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these capabilities promise improved reliability, their impact on robustness to social biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark, originally designed for Large Language Models (LLMs), to investigate the adversarial robustness of RLMs to bias elicitati...},
  note={Citations: 1}
}

@article{nag2021,
  title={Contextual BI-Directional Attention Flow With Embeddings From Language Models: A Generative Approach to Emotion Detection},
  author={Prashant Kumar Nag and Vishnu Priya R},
  journal={Advanced Robotics},
  year={2021},
  abstract={Detection of Emotions from the text is a tedious task. Presently, existing models failed to detect the emotion in absence of the emotional word in the text. The cause phrase selection which gives a deep insight into emotions is considered to be a tough task. The proposed model for detecting emotions is developed through seven layers. Initially, the dataset is represented in the Topical documents using Adversarial Topic Modelling (ATM). Convolutional Neural Network (CNN) maps each phrase in the t...},
  note={Citations: 4}
}

@article{li2024,
  title={AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization},
  author={Jiyao Li and Mingze Ni and Yifei Dong and Tianqing Zhu and Wei Liu},
  journal={arXiv preprint arXiv:2402.11940},
  year={2024},
  abstract={
 Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related modelsâ€™ robustness against adversarial attacks has not been well studied. This paper presents a novel adversarial attack strategy, attention-based image captioning attack (AICAttack), designed to attack image captioning models through subtle perturbati...},
  note={Citations: 4}
}

@article{singh2024,
  title={Adversarial Training of Retrieval Augmented Generation to Generate Believable Fake News},
  author={S. Singh and A. Namin},
  journal={BigData Congress [Services Society]},
  year={2024},
  abstract={Recent advancements in Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) and Natural Language Understanding (NLU), showcasing their ability to produce coherent and contextually relevant responses. However, their widespread use raises serious concerns regarding the potential for generating and spreading false or misleading information. This research examines the effectiveness of customized Retrieval Augmented Generation (RAG) models, alongside fine-tuned versions ...},
  note={Citations: 1}
}

@article{deng2025,
  title={Hardening LLM Fine-Tuning: From Differentially Private Data Selection to Trustworthy Model Quantization},
  author={Zehang Deng and Ruoxi Sun and Minhui Xue and Wanlun Ma and Sheng Wen and Surya Nepal and Yang Xiang},
  journal={IEEE Transactions on Information Forensics and Security},
  year={2025},
  abstract={Critical infrastructures are increasingly integrating artificial intelligence (AI) technologies, including large language models (LLMs), into essential systems and services that are vital to societal functioning. Fine-tuning LLMs for specific domain tasks are crucial for their effective deployment in these contexts, but this process must carefully address both privacy and security concerns. Without proper safeguards, such integration can introduce additional risks, such as data leakage during tr...},
  note={Citations: 1}
}

@article{ding2025,
  title={Revisiting Robust RAG: Do We Still Need Complex Robust Training in the Era of Powerful LLMs?},
  author={Hanxing Ding and Shuchang Tao and Liang Pang and Zihao Wei and Liwei Chen and Kun Xu and Huawei Shen and Xueqi Cheng},
  journal={arXiv preprint arXiv:2502.11400},
  year={2025},
  abstract={Retrieval-augmented generation (RAG) systems often suffer from performance degradation when encountering noisy or irrelevant documents, driving researchers to develop sophisticated training strategies to enhance their robustness against such retrieval noise. However, as large language models (LLMs) continue to advance, the necessity of these complex training methods is increasingly questioned. In this paper, we systematically investigate whether complex robust training strategies remain necessar...},
  note={Citations: 1}
}

@article{tekin2025,
  title={Multi-Agent Reinforcement Learning with Focal Diversity Optimization},
  author={S. Tekin and Fatih Ilhan and Tiansheng Huang and Sihao Hu and Zachary Yahn and Ling Liu},
  journal={arXiv preprint arXiv:2502.04492},
  year={2025},
  abstract={The advancement of Large Language Models (LLMs) and their finetuning strategies has triggered the renewed interests in multi-agent reinforcement learning. In this paper, we introduce a focal diversity-optimized multi-agent reinforcement learning approach, coined as MARL-Focal, with three unique characteristics. First, we develop an agent-fusion framework for encouraging multiple LLM based agents to collaborate in producing the final inference output for each LLM query. Second, we develop a focal...},
  note={Citations: 2}
}

@article{bakman2025,
  title={Reconsidering LLM Uncertainty Estimation Methods in the Wild},
  author={Y. Bakman and D. Yaldiz and Sungmin Kang and Tuo Zhang and Baturalp Buyukates and S. Avestimehr and Sai Praneeth Karimireddy},
  journal={arXiv preprint arXiv:2506.01114},
  year={2025},
  abstract={Large Language Model (LLM) Uncertainty Estimation (UE) methods have become a crucial tool for detecting hallucinations in recent years. While numerous UE methods have been proposed, most existing studies evaluate them in isolated short-form QA settings using threshold-independent metrics such as AUROC or PRR. However, real-world deployment of UE methods introduces several challenges. In this work, we systematically examine four key aspects of deploying UE methods in practical settings. Specifica...},
  note={Citations: 1}
}

@article{kim2024,
  title={Legislative review of the use and regulation of generative artificial intelligence ChatGPT: Focusing on copyright law and personal information protection legislation},
  author={Seung-Rae Kim and In-Bang Song},
  journal={Korean Institute for Aggregate Buildings Law},
  year={2024},
  abstract={With the advent of ChatGPT, the boundary between humans and machines has disappeared, making it increasingly difficult for people to discern whether the person they are dealing with is a machine or a human. Nevertheless, ChatGPT is bringing about changes in the way we work, communicate, and play in the digital world by bringing about a shift in information search and access methods and creating products such as books and art. 
ChatGPT optimizes conversations using Reinforcement Learning from Hum...},
  note={Citations: 0}
}

@article{villarreal-zegarra2025,
  title={Development, System Design, Safety, and Performance Metrics of a Conversational Agent for Reducing Depressive and Anxious Symptoms Based on a Large Language Model: The MHAI Study},
  author={David Villarreal-Zegarra and Yscenia Paredes-Gonzales and Andrea DÃ¡maso-RomÃ¡n and Judith QuiÃ±ones-Inga and G. Centeno-Terrazas and Yan Pieer and Alexis-Montalban Lozada and Montalban Lozada},
  journal={medRxiv},
  year={2025},
  abstract={None},
  note={Citations: 0}
}

@article{andreev2025,
  title={Destination (Un)Known: Auditing Bias and Fairness in LLM-Based Travel Recommendations},
  author={Hristo Andreev and P. Kosmas and Antonios D. Livieratos and A. Theocharous and Anastasios Zopiatis},
  journal={Applied Informatics},
  year={2025},
  abstract={Large language-model chatbots such as ChatGPT and DeepSeek are quickly gaining traction as an easy, first-stop tool for trip planning because they offer instant, conversational advice that once required sifting through multiple websites or guidebooks. Yet little is known about the biases that shape the destination suggestions these systems provide. This study conducts a controlled, persona-based audit of the two models, generating 6480 recommendations for 216 traveller profiles that vary by orig...},
  note={Citations: 0}
}

@article{fu2025,
  title={Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models},
  author={Wenjie Fu and Huandong Wang and Junyao Gao and Guoan Wan and Tao Jiang},
  journal={arXiv preprint arXiv:2509.24488},
  year={2025},
  abstract={As Large Language Models (LLMs) achieve remarkable success across a wide range of applications, such as chatbots and code copilots, concerns surrounding the generation of harmful content have come increasingly into focus. Despite significant advances in aligning LLMs with safety and ethical standards, adversarial prompts can still be crafted to elicit undesirable responses. Existing mitigation strategies are predominantly based on post-hoc filtering, which introduces substantial latency or compu...},
  note={Citations: 0}
}

@article{geissler2023,
  title={Analyzing User Characteristics of Hate Speech Spreaders on Social Media},
  author={Dominique Geissler and Abdurahman Maarouf and Stefan Feuerriegel},
  journal={arXiv preprint arXiv:2310.15772},
  year={2023},
  abstract={Hate speech on social media threatens the mental and physical well-being of individuals and contributes to real-world violence. Resharing is an important driver behind the spread of hate speech on social media. Yet, little is known about who reshares hate speech and what their characteristics are. In this paper, we analyze the role of user characteristics in hate speech resharing across different types of hate speech (e.g., political hate). For this, we first cluster hate speech posts using larg...},
  note={Citations: 2}
}