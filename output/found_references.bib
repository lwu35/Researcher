@article{wang2023,
  title={What Makes for Good Visual Tokenizers for Large Language Models?},
  author={Guangzhi Wang and Yixiao Ge and Xiaohan Ding and Mohan Kankanhalli and Ying Shan},
  journal={arXiv preprint arXiv:2305.12223},
  year={2023}
}

@article{zhou2024,
  title={MoE-LPR: Multilingual Extension of Large Language Models through Mixture-of-Experts with Language Priors Routing},
  author={Hao Zhou and Zhijun Wang and Shujian Huang and Xin Huang and Xue Han and Junlan Feng and Chao Deng and Weihua Luo and Jiajun Chen},
  journal={arXiv preprint arXiv:2408.11396},
  year={2024}
}

@article{au2024,
  title={An Analytical Study on Toy Age Grading Methods: A Comparative Study of Large Language Models and Machine Learning Approaches},
  author={S. L. Au and S. Mak and W. F. Tang},
  journal={Asia},
  year={2024}
}

@article{zheng2023,
  title={Large Language Models are Good Prompt Learners for Low-Shot Image Classification},
  author={Zhao-Heng Zheng and Jingmin Wei and Xuefeng Hu and Haidong Zhu and Ramkant Nevatia},
  journal={arXiv preprint arXiv:2312.04076},
  year={2023}
}

@article{xu2025,
  title={A Survey of Attacks on Large Language Models},
  author={Wenrui Xu and Keshab K. Parhi},
  journal={arXiv preprint arXiv:2505.12567},
  year={2025}
}

@article{hu2025,
  title={A Roadmap for Alignable Algorithmic Decision-Makers in the Medical Triage Domain},
  author={Brian Hu and David Chan and Taylor Sorensen and Xiusi Chen and Heng Ji and Yejin Choi and Trevor Darrell and Arslan Basharat},
  journal={Conference on Algebraic Informatics},
  year={2025}
}

@article{tsai2024,
  title={Enhance Modality Robustness in Text-Centric Multimodal Alignment with Adversarial Prompting},
  author={Yun-Da Tsai and Ting-Yu Yen and Keng-Te Liao and Shou-De Lin},
  journal={arXiv preprint arXiv:2408.09798},
  year={2024}
}

@article{xhonneux2024,
  title={Efficient Adversarial Training in LLMs with Continuous Attacks},
  author={Sophie Xhonneux and Alessandro Sordoni and Stephan Günnemann and G. Gidel and Leo Schwinn},
  journal={arXiv preprint arXiv:2405.15589},
  year={2024}
}

@article{chen2025,
  title={CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs},
  author={Sijia Chen and Xiaomin Li and Mengxue Zhang and Eric Hanchen Jiang and Qin Zeng and Chen-Hsiang Yu},
  journal={arXiv preprint arXiv:2505.11413},
  year={2025}
}

@article{rennard2024,
  title={Bias in the Mirror : Are LLMs opinions robust to their own adversarial attacks ?},
  author={Virgile Rennard and Christos Xypolopoulos and M. Vazirgiannis},
  journal={arXiv preprint arXiv:2410.13517},
  year={2024}
}

@article{luo2024,
  title={Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation},
  author={Yucong Luo and Qitao Qin and Hao Zhang and Mingyue Cheng and Ruiran Yan and Kefan Wang and Ouyang Jie},
  journal={arXiv preprint arXiv:2412.18176},
  year={2024}
}

@article{yi2025,
  title={Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks},
  author={Xin Yi and Yue Li and Linlin Wang and Xiaoling Wang and Liang He},
  journal={arXiv preprint arXiv:2501.10639},
  year={2025}
}

@article{song2025,
  title={JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models},
  author={Jiaxin Song and Yixu Wang and Jie Li and Rui Yu and Yan Teng and Xingjun Ma and Yingchun Wang},
  journal={arXiv preprint arXiv:2505.19610},
  year={2025}
}

@article{che2025,
  title={Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities},
  author={Zora Che and Stephen Casper and Robert Kirk and Anirudh Satheesh and Stewart Slocum and Lev E McKinney and Rohit Gandikota and Aidan Ewart and Domenic Rosati and Zichu Wu and Zikui Cai and Bilal Chughtai and Yarin Gal and Furong Huang and Dylan Hadfield-Menell},
  journal={arXiv preprint arXiv:2502.05209},
  year={2025}
}

@article{wang2025,
  title={Adversarial Preference Learning for Robust LLM Alignment},
  author={Yuanfu Wang and Pengyu Wang and Chenyang Xi and Bo Tang and Junyi Zhu and Wenqiang Wei and Chen Chen and Chao Yang and Jingfeng Zhang and Chaochao Lu and Yijun Niu and Keming Mao and Zhiyu Li and Feiyu Xiong and Jie Hu and Mingchuan Yang},
  journal={arXiv preprint arXiv:2505.24369},
  year={2025}
}

@article{ye2024,
  title={TL-Training: A Task-Feature-Based Framework for Training Large Language Models in Tool Use},
  author={Junjie Ye and Yilong Wu and Sixian Li and Yuming Yang and Tao Gui and Qi Zhang and Xuanjing Huang and Peng Wang and Zhongchao Shi and Jianping Fan and Zhengyin Du},
  journal={arXiv preprint arXiv:2412.15495},
  year={2024}
}

@article{pellert2024,
  title={AI Psychometrics: Assessing the Psychological Profiles of Large Language Models Through Psychometric Inventories},
  author={Max Pellert and Clemens M. Lechner and Claudia Wagner and Beatrice Rammstedt and Markus Strohmaier},
  journal={Perspectives on Psychological Science},
  year={2024}
}

@article{he2023,
  title={A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics},
  author={Kai He and Rui Mao and Qika Lin and Yucheng Ruan and Xiang Lan and Mengling Feng and Erik Cambria},
  journal={arXiv preprint arXiv:2310.05694},
  year={2023}
}

@article{buyl2024,
  title={Large Language Models Reflect the Ideology of their Creators},
  author={Maarten Buyl and Alexander Rogiers and Sander Noels and Iris Dominguez-Catena and Edith Heiter and Raphaël Romero and Iman Johary and A. Mara and Jefrey Lijffijt and T. D. Bie},
  journal={arXiv preprint arXiv:2410.18417},
  year={2024}
}

@article{malmqvist2024,
  title={Sycophancy in Large Language Models: Causes and Mitigations},
  author={Lars Malmqvist},
  journal={arXiv preprint arXiv:2411.15287},
  year={2024}
}

@article{jandaghi2023,
  title={Faithful Persona-based Conversational Dataset Generation with Large Language Models},
  author={Pegah Jandaghi and XiangHai Sheng and Xinyi Bai and Jay Pujara and Hakim Sidahmed},
  journal={arXiv preprint arXiv:2312.10007},
  year={2023}
}

@article{zhang2024,
  title={Pretraining Data Detection for Large Language Models: A Divergence-based Calibration Method},
  author={Weichao Zhang and Ruqing Zhang and Jiafeng Guo and M. D. Rijke and Yixing Fan and Xueqi Cheng},
  journal={arXiv preprint arXiv:2409.14781},
  year={2024}
}

@article{zhou2024,
  title={A Simple yet Effective Training-free Prompt-free Approach to Chinese Spelling Correction Based on Large Language Models},
  author={Houquan Zhou and Zhenghua Li and Bo Zhang and Chen Li and Shaopeng Lai and Ji Zhang and Fei Huang and Min Zhang},
  journal={arXiv preprint arXiv:2410.04027},
  year={2024}
}

@article{jiang2024,
  title={RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking},
  author={Yifan Jiang and Kriti Aggarwal and Tanmay Laud and Kashif Munir and Jay Pujara and Subhabrata Mukherjee},
  journal={arXiv.org},
  year={2024}
}

@article{hattab2024,
  title={Persona Adaptable Strategies Make Large Language Models Tractable},
  author={Georges Hattab and Aleksandar Anžel and Akshat Dubey and Chisom Ezekannagha and Zewen Yang and B. Ilgen},
  journal={International Conference on Natural Language Processing and Information Retrieval},
  year={2024}
}