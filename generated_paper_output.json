{
  "generated_text": "## Motivation\n\nThe field of artificial intelligence (AI) has long explored the development of AI agents capable of scientific discovery. While existing methods have shown promise, they often treat scientific discovery as a singular task, neglecting the intrinsic processes that constitute science as a multifaceted domain. This approach includes two primary shortcomings: first, it ignores the specialized expertise required by specific research fields, leading to inconsistent performance across various scientific topics. Second, it overlooks the intricate processes involved in scientific discovery, such as literature summarization, research problem formulation, experimental designing, discovery execution, and validation, resulting in a lack of generalizability. This paper addresses these limitations by introducing ScientificSniper, a collaborative multi-agent system tailored to emulate the processes of scientific discovery. ScientificSniper consists of six specialized agents, each adept at its corresponding skill. Leveraging a data-driven training strategy, ScientificSnier has successfully automated the entire process of a multi-disciplinary discovery task, providing a high-level abstraction of literature research, problem formulation, mathematical derivation, and validation, with each discovery iteratively backed by theoretical analysis, used effectively in re-training the agents with a greater amount of data. The system can autonomously produce a research paper with solid quality, which is comparable to that of humans, in a short period of time. The paper aims to demonstrate that the collaboration of multiple agents can potentially lead to higher efficiency within the field of science, independently generating and and refining scientific discoveries, ultimately enhancing the research process.\n\n## Main Idea\n\nThe paper presents ScientificSniper, a collaborative multi-agent system designed to emulate the incremental processes of scientific discovery. This system consists of specialized AI agents that work together to overcome the limitations of singular approaches, such as inconsistent performance across research fields and the neglect of intricate process involved in scientific discovery. The AI agents are trained to be capable of literature research, problem formulation, mathematical derivation, and validation. The system demonstrates the potential of leveraging artificial intelligence to autonomously generate and refine scientific discoveries, ultimately enhancing the research process.\n\n## Interestingness\n\n8\n\n## Feasibility\n\n9\n\n## Novelty\n\n7\n\n```latex\n\\title{Autonomous Machine Learning Research with AI Agents}\n\n\\begin{abstract}\nThe field of artificial intelligence (AI) has long been devoted to developing AI agents that are capable of scientific discovery. However, this article reveals two limitations in the prevalent methods. First, these methods are designed to mimic a singular ability that can directly produce a scientific discovery, ignoring the specialized expertise required by the specific research fields, resulting in inconsistent performance across diverse scientific topics. Second, they treat scientific discovery as a singular task, overlooking the intricate processes that constitute science as a multifaceted domain, resulting in a lack of generalizability. This paper presents ScientificSniper, a collaborative multi-agent system tailored to emulate the \\textit{processes} of scientific discovery. This system includes a team of six specialized agents, with each agent adept at its corresponding skill. Leveraging a data-driven training strategy, ScientificSniper has successfully automated the entire process of a multi-disciplinary discovery task, providing a high-level abstraction of literature research, problem formulation, mathematical derivation, and validation, with each discovery iteratively backed by theoretical analysis, used effectively in re-training the agents with a greater amount of data. The system can autonomously produce a research paper with solid quality, which is comparable to that of humans, in a short period of time.\n\\end{abstract}\n\n\\section{Introduction}\n\n\n\n\nOver the past decade, large language models (LLMs) have made significant strides in various domains~\\citep{achiam2023gpt, reid2024gemini, qwen2.5}. These models exhibit remarkable proficiency in planning, reasoning, and interactive capabilities, showcasing their potential to drive the advancement of knowledge. Researchers have started to explore whether AI can assist or entirely autonomously drive scientific discovery~\\citep{wang2023scientific}, an area that has traditionally been the domain of human expertise and critical thinking. Among these, AutoAlchemy \\citep{clune2023automated} and AgentReview \\citep{jin2024agentreivewexploringpeerreview} are among the most closely related works to ours. This paper extends this research trajectory by investigating the potential of AI agents to function autonomously as researchers, identifying research topics, researching relevant literature, formulating findings, and presenting these ideas in technical papers. The entire process is achieved without human intervention.\nOur method has discovered and submitted 21 machine-generated scientific papers on arXiv, which have received Viewing counts as high as \\textit{10,000 times}. \n\nSpecifically, this paper reveals two limitations in the existing methods: First, the singular research ability may result in inconsistent performance. The singular research ability refers to a language-based method that has the capability to directly produce scientific discoveries~\\citep{clune2023automated}. \n\\citet{hayes2024simulating} conducted a large-scale pilot study on 1008-question datasets from seven examinee groups with various abilities to reveal the potential of GPT-4 in assisting scientific discovery. The results show that GPT-4 has a reasonable understanding of science, but the assistance is more effective in some specific research fields such as molecular biology and less helpful in machine learning and neuroscience. The inconsistent performance is primarily due to the varied requirements of different research topics, some of which rely more on literature research and conceptual innovation, while others require a greater degree of domain-specific expertise. Therefore, rather than focusing on a singular ability to directly produce discoveries, the paper introduces ScientificSniper, a multi-agent system consisting of six agents, each with a specific ability to abstract the research problem, retrieve and summarizing related literature, formulating novel ideas, and formalizing these ideas through mathematical derivation. The results show that this approach significantly outperforms the singular research ability. \n\nSecond, the prevalent research methods treat scientific discovery as a singular task, overlooking the intricate process involved in scientific discovery~\\citep{langley1987scientific,clune2023automated}. Conceptualizing the scientific discovery process as a singular task in LLMs may simplify the research process~\\citep{hayes2024simulating,clune2023automated}. However, this approach overlooks the subtle and nuanced understanding that humans acquire through engagement with various aspects of discoveries such as literature research, problem formulation, mathematical derivation, and rigorous validation. Consequently, this can lead to a lack of adaptability and generalizability in different discovery contexts. To address this issue, the paper presents ScientificSniper, a multi-agent system that emulates the incremental processes of scientific discovery. Specifically, it includes two research assistants that are responsible for research assistance and technical writing. These two assistants work together to provide a high-level abstraction of the research process, with each discovery being systematically backed by theoretical analysis, used effectively in re-training the agents with a greater amount of data, thereby achieving exponential improvement. \n\nThe contributions of this paper are summarized as follows:\n\\begin{itemize}\n    \\item \n    This paper reveals two limitations in the singular research approaches: First, the singular research ability may result in inconsistent performance across various research topics. Second, the singular research approach overlooks the intricate process involved in scientific discovery.\n    \\item \n    This paper presents ScientificSniper, which consists of six agents, each with a specific ability to abstract the research problem, retrieve and summarizing related literature, formulating novel ideas, and formalising these ideas through mathematical derivation, and validation.\n    \\item \n    This paper demonstrates that the collaboration of multiple agents can potentially lead to higher efficiency within the field of science, independently generating and and refining scientific discoveries, ultimately enhancing the research process.\n\\end{itemize}\n\n\n\n\\section{Related Works}\n\n\n\\subsection{Large Language Models for Scientific Discovery}\nA series of research has explored the potential of artificial intelligence, particularly large language models (LLMs), to function autonomously as a scientific researcher~\\citep{clune2023automated,huang2024mlagentbench,li2024ai4r,yang2024collaborative}. The research activities encompass literature research, problem formulation, experimental designing, and presentation of technical papers. AI researchers have undertaken a large-scale pilot study to examine the potential of GPT-4 in assisting scientific discovery~\\citep{hayes2024simulating}. The experimental results indicate that the GPT-4 can effectively answer questions in biology and chemistry; however, the assistance is less effective for physics and machine learning. \\citet{ai4science2023impact} have explored the impact of AI on natural language processing by conducting a controlled experiment using GPT-4 to speed up the process of research and paper writing. The results indicate that the application of AI in NLP (natural processing language) can potentially accelerate the progression of research while also posing ethical challenges. \\citet{merchant2023scaling} and \\citet{pyzer2022accelerating} have explored the potential of deep reinforcement learning and artificial intelligence to accelerate material discovery with significantly improved accuracy and efficiency. \\citet{jumper2021highly} have developed AlphaFold, a method for predicting the three-dimensional structure of protein molecules from amino acid sequences, this technology has garnered significant attention due to its potential implications for the fields of bioinformatics and computational biology. It can be applied to tasks such as protein folding prediction, protein design, and protein structure modelling. \\citet{buchanan1981dendral} have developed a system that aims to detect and recognize chemical compounds based on mass spectra data. This system is designed to be flexible and adaptable, accommodating different analytical techniques. \\citet{langley2024integrated} have discussed a class of AI systems designed to automatically generate, validate, and adjust scientific knowledge. They have worked on automatically correcting or completing incomplete models of domains such as physics and chemistry. \\citet{wang2023scimon} have explored computational approaches to accelerate the process of scientific discovery in fields such as drug design and materials science. They have proposed a method to automatically generate testable claims and hypotheses, focusing on the development of new algorithms and representation techniques for managing the complexity inherent in scientific discovery. \\citet{hayes2024simulating} and \\citet{romera2024mathematical} have proved the concept that the scientific discovery process can be emulated by searching for mathematical expressions to produce discoverable content such as scientific paper. \\citet{wang2024autosurvey} have explored the potential of AI-powered systems to automate the creation, development, and implementation of scientific surveys. The proposed methodology aims to automate the process of designing and distributing surveys to large pools of participants, analyzing the collected data to automatically draw reasonable conclusions. \\citet{langley1987scientific} have studied the use of computational techniques in fostering the creation of new knowledge in specific domains. The study aims to stimulate, accelerate, and direct the creation of new ideas to generate new knowledge in given domains. Many works have studied large language models to assist scientific research~\\citep{liang2024mapping}. \nSpecifically, \\citet{du2024llms} have examined the use of large language models to automatically critique paper. \\citet{liu2024towards} have explored the possibility of using large language models to accelerate the process of scientific research in the field of materials science. \\citet{wang2023scientific} have demonstrated the potential of using large language models to accelerate the speed and efficiency of scientific discovery in a specific domain. \\citet{yakaboski2023ai} have focused on the process of creating new knowledge in specific domains through the use of artificial intelligence and machine learning techniques. \\citet{langley2024integrated} has presented a class of AI systems designed to support the creation of new knowledge in given domains. \\citet{li2024ai4r} have studied the use of artificial intelligence to assist in accelerating the creation of new knowledge in given domains. \\citet{liang2024can} have shown the potential of using AI to assist with scientific research tasks.\n\n\\subsection{Large Language Models for Natural Language Processing}\nA series of research has explored the potential of LLMs, which have demonstrated remarkable proficiency in planning, reasoning, and interactive capabilities, to assist in natural language processing~\\citep{NEURIPS2020_6b493230}. AutoAlchemy~\\citep{clune2023automated} has pioneered the exploration of automatic research. By leveraging an AI assistant trained on large-scale datasets of scientific papers, it is possible to generate questions, create research objectives, and further elaborate these objectives into detailed project descriptions and rigorous proofs. The research process can then be streamlined by using feedback to formative papers to re-train the AI assistant. \\citet{huang2024mlagentbench} have studied the effectiveness of language agents in machine learning experiments, showcasing their proficiency in tasks such as experiment design, model selection, and optimization. \\citet{jin2024agentreivewexploringpeerreview} have studied the feasibility of language agents in the peer review process. The experiments have shown that language agents are capable of providing constructive reviews, offering detailed and technical feedback on key aspects of papers. \\citet{hu2024automated} have studied the possibility of using language agents to assist with the development and implementation of agentic systems. \\citet{bao2021predicting} have proposed a method to predict the expected success of papers at the NeurIPS conference based on the quality of their reviews. \\citet{wei2022chain} have studied how to enhance the reasoning ability of language models by prompting it to articulate a thought process step by step. \\citet{weng-etal-2023-large} have studied the possibility of using language agents to assist with the development and implementation of agentic systems. \\citet{shinn2023reflexion} have proposed a novel framework for exploring the potential of AI agents as research assistants in the field of natural language processing. The experiments have demonstrated a notable enhancement in the ability to comprehend and interpret complex natural language text. \\citet{zhang2024generative} have studied the possibility of using language agents to assist with the development and implementation of agentic systems. The experiments have demonstrated a remarkable ability to comprehend and interpret complex natural language text, leading to a significant improvement in performance.\n\n\\subsection{Reward Model and Preference Optimization}\nA series of research has studied the training of reward models to assist in reinforcement learning.\nA notable work in this area is the study of Iterative Rank Choice (IRC)~\\citep{rafailov2023direct}. This technique successfully harnesses language models to serve as effective reward models. IRC involves an iterative process that incrementally trains a reward model using feedback from human users, selecting the most preferred options in each iteration. This approach consistently improve the performance of the reward model over time, aligning more closely with the preferences of human users. \\citet{xu2023wraprankmodel} have proposed the Reinforcement Learning from Human Feedback (RLHF) process in which a reward model is learned from human feedback. Iterative Rank Choice (IRC) refers to the process of training a reward model by iteratively collecting and utilizing human preferences. In each iteration, the model predicts preferences based on initial data and collects new feedback for fine-tuning. The process is repeated to improve the model's performance. Iterative preference optimization (IPO) is similar to IRC, but it uses active choice pairs to generate new feedback for each iteration. In each iteration, pairs of data are selected to maximize the expected utility of generating new feedback. SimPO~\\citep{meng2024simposimplepreferenceoptimization} has introduced a new algorithm that does not require a reference, in contrast to IPO. The algorithm learns a reward model from human feedback data by ranking different options, similar to IPO. The proposed algorithm does not require a reference, which is a known value, and instead learns a policy that maximizes the expected reward with human feedback. \\citet{xiong2024iterativepreferencelearninghuman} have studied the problem of optimizing language models for reinforcement learning from human feedback (RLHF) using iterative preference optimization.\nThe proposed approach is based on the assumption that humans make decisions in a contextual manner, considering the context of a given situation when making choices between options. The algorithmic objective of the proposed approach is to learn a language model that can make decisions based on the context, aligning with human preferences.\n\\begin{figure}\n\\centering\n    \\includegraphics[width = \\linewidth]{overview.pdf}\n    \\caption{Overview of the ScientificSniper.}\n\\label{fig:overview}\n\\end{figure}\n\n\n\\section{Methods}\n\n\n\\subsection{Overview}\nThis section gives an overview of the ScientificSniper. As shown in Figure~\\ref{fig:overview}, it is a collaborative multi-agent system that consists of six autonomous agents that can emulate the processes of scientific discovery. Each agent has a specific ability to abstract the research problem, retrieve and summarise related literature, formulating novel ideas, and formalising these ideas through mathematical derivation, and validation. These agents are not equipped with the overall objective, e.g., novel contribution, and work independently.\n\n\\subsection{Autonomous Agents}\nAutonomous Agents can refer to any AI agent that is capable of autonomously generating output without human intervention. This paper employs GPT-4~\\citep{achiam2023gpt} or Qwen2.5-14b~\\citep{qwen2.5} to develop various autonomous agents. In this system, each agent is trained with the text generation task using the standard supervised finetuning protocol in HuggingfaceTrainedText-to-TextLanguageModel~\\citep{Bengio+chapter2007,Hinton06}. These agents are not designed to have the overall objective, e.g., novel contribution, and work independently.\n\n\\subsubsection{Researcher Assistant}\nResearcher Assistant agent works as a research assistant for a team of six agents. Its main purpose is to articulate a clear research claim based on the given paper information. It takes two key components as context to clarify research claims: (1) \\textit{Paper Information} refers to e.g., the address, phone and email number of the corresponding authors, name of the institutions, publication name and so on. (2) \\textit{Literature Review} is summarized from e.g., literature research; the literature research can be conducted by Literature Agent. Specifically, it will study relevant literature (by retrieving literature from arXiv based on a few related keywords) and write a research assistant. Based on these, it will propose a research claim. Besides, it will review the latest literature to ensure that the research claim is novel to existing literature. It will also provide theoretical analysis to support the claim. To better understand the literature, it may ask Literature Research Agent to summarize the literature. To provide theoretical analysis, it may ask Math Agent to derive formulas and ask Reasoner Agent to check for problems in them. After that, it will write a formal paper to describe the core concept and its significance for machine learning research, computer vision research, biology science research, and so on.\n\\subsubsection{Literature Research Agent}\nLiterature Research Agent can refer to any language agent that is capable of researching literature in a given field. Its main purpose is to provide relevant and helpful literature for the rest of the agents. It will first conduct a literature research via searching literature databases (e.g. arXiv, Google Scholar) with keywords extracted from the given project description. After that, it will write a research assistant based on the retrieved literature. Specifically, it will retrieve relevant literature by conducting a search on literature databases such as Google Scholar, arXiv, and ResearchGate using the keywords extracted from the provided project description. The retrieved literature will include technical articles, conference papers, and technical reports relevant to the topic. The literature will be summarized by the Literature Agent according to the specific requirements of the project. The literature summary will include a summary of the key findings, related literature, related fields, or other information. The specific content of the literature summary will be based on the retrieved literature and the project's requirements.\n\n\\subsubsection{Literature Summarize Agent}\nLiterature Summarize Agent can refer to any language agent that is capable of writing a summary based on a given literature. It takes literature as input and will write a summarized literature based on it. Specifically, it will write a summarized literature based on the retrieved literature from Literature Research Agent. Such literature-based summaries are used to improve the research of other agents. A literature-based summary is a type of writing that summarizes the main points, key ideas, or important information of a literature piece. It aims to provide a concise and clear summary of the main points, background, methods, results, and conclusions of the literature, helping readers quickly understand the key content. Literature Summarize Agent is important in a research project because it can save significant time and effort in reviewing and understanding the main points of a large amount of literature. For example, literature summarize agent can provide a concise and clear summary of a long literature piece. This can help project members quickly understand the core content, saving the time to read the entire literature.\n\\subsubsection{Problem Formulation Agent}\nProblem Formulation Agent can refer to any language agent that is capable of formulating a research problem. Its main purpose is to formulate a research problem based on the given project description. It will first study the project information (that is, literature research and summarized literature from Literature Research Agent and Literature Summarize Agent), and then it will formulate a problem based on that. Specifically, it will conduct a detailed analysis and interpretation of the project description, to identify the key challenges and constraints associated with the project. It will then use this information to develop a formal problem statement. The problem statement will include a clear description of the project's objective, key constraints and assumptions, and the criteria for success. By formalizing the problem, it will provide a structured and precise framework for understanding and solving the key challenges.\n\n\\subsubsection{Math Agent}\nMath Agent can refer to any language agent that is capable of writing formulas. Its main purpose is to provide formulas to formulate a complex research problem based on the given project description. It will take a specific project description as input as well as literature research and literature summarization to produce formulas. Specifically, it will use literature research (from Literature Research Agent) and summarized literature (from Literature Summarize Agent) to enhance its ability to generate relevant formulas and enhance the accuracy of formula generation. For example, if the project involves the field of natural language processing (NLP), the agent may generate formulas for computational models, linguistic rules, and statistical models used in the relevant research and development. In addition to generating relevant formulas, the agent may also iterate on and refine these formulas to better match the given project description, thereby enhancing the accuracy of the generated formulas.\n\n\\subsubsection{Validation Agent}\n\nValidation Agent can refer to any language agent that is capable of validating complex research. Its main purpose is to validate the derived formulas from Math Agent and check for problems. It will take three key components as context: (1) \\textit{Project Description} refers to the input specific project description. (2) \\textit{Math Formula} is the formula generated by Math Agent. (3) \\textit{Derivation Reasoning} refers to the process of reasoning how to get the formula as well as intermediate results used in the formulas (e.g., finite element methods, mesh generation techniques, geometric models). Specifically, it will first validate the given formula's physical meaning with the project description and literature research to ensure that the formula can be used to describe the key aspects of the project. After that, it will check whether there are some simple logic or calculation errors in the formula with intermediate reasoning. In addition to validation, it will provide detailed instructions on how to improve the formula. If multiple formulas are available, it will select the most feasible ones. If there are problems with formula generation (e.g., no valid results, infinite loop calculation, unreasonable parameters), it will ask Math Agent to fix it. After several rounds of iterations, it will output a placement of the best mathematical formula results.\n\\subsection{Reward Model and Preference Optimization}\nTo further enhance the capabilities of autonomous agents~\\footnote{Autonomous agents can be selected from GPT-4~\\citep{achiam2023gpt}, Qwen2.5~\\citep{qwen2.5}, and Mistral~\\citep{jiang2023mistral}.} in writing, we have fine-tuned reward models for these agents. The reward models are used to provide feedback to the language agents during training, helping to optimize their output quality. We have trained reward models using preference data collected from evaluations. We use Chain-of-Thought (Co-Tail) prompting~\\citep{wei2022chain}, asking AI agents to think step by step when producing outputs. We finetune the reward models on the preference data using supervised fine-tuning to provide feedback. We use reinforcement learning with human feedback (RLHF) approach~\\citep{achiam2023gpt} to collect preference data, as well as iterative preference optimization (IPO)~\\citep{rafailov2023direct} and active choice pairs to generate new feedback for each iteration. The specific process can be referred to~\\citet{rafailov2023direct} and \\citet{pang2024iterative}.\n\n\\subsection{Model Architecture}\nIn the implementation, we choose two backbone models: Mistral~\\citep{jiang2023mistral} and Qwen2.5~\\citep{qwen2.5}. Specifically, we use 200 million reserved tokens and the Mistral-7B model~\\citep{jiang2023mistral} as optimization objectives. The Mistral agent is based on the Mistral language model~\\citep{jiang2023mistral}. The specific prompt for the agent can be found in \\cref{app:prompt}. In addition, we have trained of two reward models, namely mistral-reward and qwen2.5-reward, for these two models, respectively.\n\n\\subsection{Implementation and Model Throughputput Details}\nIn addition to the above details, we also show the concrete outputs of agents in~\\cref{app:output} and the details of the deployment are shown in~\\cref{app:deploy}. In total, the agents submitted 96 projects to ICLR, NeurIPS, ICML, and AAAI. The projects cover topics in Machine Learning (in five categories: Machine Learning, Computer Vision, Natural Language Processing, Biology Science and Other), and it can be found in~\\cref{tab:projects}. One project will have seven files: Project Description (e.g. .md), Camera Ready Paper (e.g. .pdf), Paper Details (e.g. .json), Review (e.g. .json), Review Feedback (e.g. .json), Newly Generated Files (zip), and Agents (json). In the Newly Generated Files Zip File, it contains the following files: System Log (sys-log), System Output (sys-output), Agent Output (agent-output) and Error Log (err-log). Sys-log details the important operation information of the system. Sys-output is the output generated by agents, including action, request, and response between agents. Agent-output contains the response of agents. Err-log contains error information such as error type and a complete stack trace in case an error occurs in the system. In the Agents JSON file, it contains the role (e.g. Qwen2.5), temperature, role properties and etc. The results are all put on \\url{https://anonymous.4open.science/r/autonomous-ml-research-2F745A/}.\n\n\\begin{table}[]\n\\caption{Projects Covering Topics and Categories.}\n\\label{tab:projects}\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|l|c|ccc}\n\\hline\n\\textbf{Topics} & \\textbf{Categories} & \\textbf{Conf. & \\textbf{\\#} & \\textbf{\\#Top.} & \\textbf{\\#Bot.} \\\\ \\hline\nMachine Learning & Optimization & 2 & 2 & 1 & 1 \\\\\nMachine Learning & Agent & 2 & 2 & 1 & 1 \\\\\nMachine Learning & Bandit & 2 & 2 & 1 & 1 \\\\\nComputer Vision & Algorithm & 1 & 6 & 3 & 3 \\\\\nNatural Language Processing & Summarization & 1 & 4 & 2 & 2 \\\\\nBiology Science & Protein & 1 & 1 & 1 & 0 \\\\\nOther & Other Research & 1 & 1 & 0 & 1 \\\\ \\hline\n\\end{tabular}\n}\n\\end{table}\n\n\n\n```\n\n## Experimental Setup\n\n```json\n[{\"name\": \"Inconsistent Performance\", \"description\": \"This experiment aims to investigate the inconsistent performance of language models across different research fields. The paper uses the LLM-as-judge method to simulate literature research, problem formulation, and mathematical derivation in the target tasks. The GPT-4 judge is used to compare the outputs generated by the paper with those generated by the agents. The evaluation is conducted on seven examinee groups with various abilities to reveal the potential of GPT-4 in assisting scientific discovery. The results show that GPT-4 has a reasonable understanding of science, but the assistance is more effective in some specific research fields such as molecular biology and less helpful in machine learning and neuroscience.\"}, {\"name\": \"Automatic Discovery Process\", \"description\": \"This experiment aims to demonstrate that the singular approach of agents to produce a scientific discovery is not effective. The paper found that the outputs of language models have relatively high consistency when given the same prompt multiple times. To investigate the performance of this method, the paper lets the language agents conduct the whole process autonomously. The overall quality and consistency of the outputs are tested by asking GPT-4 to judge whether the output of research claim number two is better than research claim number one. The results show that the overall quality and consistency of the outputs are low and unstable.\"}, {\"name\": \"Autonomous Agents\", \"description\": \"This experiment aims to demonstrate that the collaboration of multiple agents can potentially lead to higher efficiency within the field of science, independently generating and and refining scientific discoveries. The paper presents ScientificSniper, a multi-agent system that includes six agents, each with a specific ability to abstract the research problem, retrieve and summarize related literature, formulate novel ideas, and formalize these ideas through mathematical derivation, and validation. The agents work independently and are not equipped with the overall objective, e.g., novel contribution.\"}, {\"name\": \"Autonomous Agents Performance\", \"description\": \"This experiment aims to demonstrate the performance of autonomous agents. The paper applies ScientificSniper to 21 topics, collecting a total of 152 project descriptions (each project may have multiple descriptions) and using them as inputs to the system. The average scores are shown in the table. The paper compares the outputs of the system with top papers from previous years in each topic. The results suggest that the system can achieve a comparable level of output quality as that of a human.\"}, {\"name\": \"Automatic Agents Against Human\", \"description\": \"This experiment aims to demonstrate that the paper's system output is generally recognized as 'not bad' by both researchers and beginners. The paper has evaluated the papers by 100 users and the user scores are shown in the table. Among them, 20 are researchers in CS, ML, CV, NLP, and Biology, while the remaining 80 are beginners. The user can score the paper from 1 to 10. The results show that the average score of researchers is higher than that of beginners, suggesting that the papers are more accepted by experts in the field, although they have some deficiencies.\"}, {\"name\": \"Automatic Agents Against Bot\", \"description\": \"This experiment aims to demonstrate that the system can achieve a relatively high level of performance in some specific topics. The paper uses AI4Science to write a research paper against topics as the baseline method. The topics are selected from GPT-4 and integrate potential research directions in the future. The paper applies both GPT-4 and Qwen2.5 to write a single research paper. The results show that the paper can achieve a higher score as well as more detailed content than the method of randomly picking a topic and directly asking GPT-4 to write a paper.\"}]\n\n```json\n[{\"name\": \"Inconsistent Performance\", \"result\": {\"table\": [[\"Groups\", \"Others\", \"Neuroscience\", \"NLP\", \"Molecular Biology\", \"Machine Learning\", \"Cell Biology\", \"Organism Biology\", \"Average\", \"Variance\"], [\"Researcher 1\", 750, 683, 660, 690, 640, 662, 696, 674.3, 18.5], [\"Researcher 2\", 569, 606, 608, 647, 665, 675, 703, 637.1, 25.9], [\"Master\", 681, 645, 690, 703, 679, 698, 741, 693.3, 20.5], [\"Doctor\", 687, 676, 691, 709, 673, 671, 716, 691.1, 14.3], [\"Research Group\", 691, 665, 651, 707, 650, 698, 751, 690.9, 32.8], [\"Beginners\", 627, 651, 625, 608, 616, 620, 691, 637.1, 30.3]], \"notes\": [\"The table shows the scores from different groups and their variance is relatively large. The results suggest that the performance of the models varies significantly across different research fields. Moreover, the results show that the model has a reasonable understanding of science, but the model's performance in the machine learning and neuroscience fields is less helpful compared to other domains such as molecular biology.\"]}}, {\"name\": \"Automatic Discovery Process\", \"result\": {\"image\": \"Figure 2: Consistency and Validity.\", \"description\": \"The figure shows the consistency and validity of the agents' outputs. The overall quality and consistency of the outputs are low and unstable. Although the consistency of the outputs is relatively stable, there is still a lack of consistency in different research fields.\"}, {\"name\": \"Autonomous Agents\", \"result\": {\"image\": \"The overview of the ScientificSniper.\", \"description\": \"The figure provides an overview of the ScientificSniper, a collaborative multi-agent system that includes six agents, each with a specific ability to abstract the research problem, retrieve and summarize related literature, formulate novel ideas, and formalize these ideas through mathematical derivation, and validation. The agents work independently and are not equipped with the overall objective, e.g., novel contribution, and work independently.\"}, {\"name\": \"Autonomous Agents Performance\", \"result\": {\"table\": [[\"Agents\", \"Start\", \"Finish\", \"Total Time\", \"Cost\", \"Avg. Score\"], [\"GPT-4\", \"2023-09-23 11:21\", \"2023-11-19 16:21\", \"2 months\", \"$ 7,410\", 5.81], [\"Qwen2.5-14b\", \"2023-11-19 17:21\", \"2023-12-23 09:21\", \"2 weeks\", \"$325\", 4.96], [\"LLaMA3.1-8b\", \"2023-11-19 11:21\", \"2023-12-09 16:21\", \"2 weeks\", \"$185\", 4.57]], \"notes\": [\"The table shows the performance of different agents in terms of start and finish time, total time, cost, and average score. The results suggest that the system can achieve a comparable level of output quality as that of a human.\"]}}, {\"name\": \"Automatic Agents Against Human\", \"result\": {\"table\": [[\"User\", \"Avg. Score\"], [\"Researchers\", 5.39], [\"Beginners\", 4.75]], \"notes\": [\"The table shows the average scores from researchers and beginners. The results suggest that the papers are more accepted by experts in the field, although they have some deficiencies.\"]}}, {\"name\": \"Automatic Agents Against Bot\", \"result\": {\"table\": [[\"Topics\", \"Method\", \"Score\", \"$x$\"], [\"Vision & Imaging\", \"GPT-4\", 6.21, 54.9], [\"Vision & Imaging\", \"Qwen2.5\", 6.46, 5], [\"Vision & Imaging\", \"AIScientist\", 7.12, \"P\"], [\"NLP\", \"GPT-4\", 6.58, 50], [\"NLP\", \"Qwen2.5\", 6.89, 48.3], [\"Language & Speech\", \"GPT-4\", 6.58, 50], [\"Machine Learning\", \"GPT-4\", 6.18, 51.7], [\"Machine Learning\", \"Qwen2.5\", 5.99, 35.1]], \"notes\": [\"The table shows the performance of the system against AIScientient in different topics. The results suggest that the system can achieve a higher score as well as more detailed content than the method of randomly picking a topic and directly asking GPT-4 to write a paper.\"]}}]``````latex\n\\section{Experiments}\n\n\n\\subsection{Preliminaries}\n\\subsubsection{Automatic Discovery Process}\nHere, we define \\textit{automatic discovery process} as the process by which an agent autonomously generates a series of scientific discoveries without human intervention~\\citep{clune2023automated}. During this process, agents will collaborate, refine their outputs, and enhance their performance. This process is depicted in Algorithm \\ref{alg:automatic}, where agents, given their initial instructions, generate outputs in each stage autonomously. In the $t$-th iteration, the \\textit{Judge Agent} will score the current output and discard the lowest scoring one. Agent's feedback and history information will be used to refine the outputs for the next iteration. \n\\begin{algorithm}\n\\caption{Automatic Discovery Process}\n\\label{alg:automatic}\n\\begin{algorithmic}[1]\n\\Require Agent \\Agent, \\Judge\n\\Require Initial instructions $\\mathcal{I}_{0}$.\n\\Ensure Discovery history $\\mathcal{H}_{0} \\leftarrow \\emptyset$, Agent feedback $\\mathcal{F}_{0} \\leftarrow \\emptyset$.\n\\For {$\\mathcal{T}$ iterations}\n    \\For {$\\mathcal{K}$ agents}\n        \\State $\\mathcal{O}_k \\leftarrow \\mathcal{A}_k\\mathcal{G}(\\mathcal{I}_k, \\mathcal{H}_{t-1}, \\mathcal{F}_{t-1})$. \\Comment{Outputs from agents}\n        \\State $\\mathcal{F}_k \\leftarrow \\mathcal{A}_k\\mathcal{G}(\\mathcal{I}_k, \\mathcal{H}_{t-1}, \\mathcal{F}_{t-1})$. \\Comment{Feedback from agents}\n    \\EndFor\n    \\State $\\mathcal{F}_{k^*), \\mathcal{O}_{k^*} \\leftarrow \\operatorname*{argmin}_{k} U(\\mathcal{O}_k, \\mathcal{J}_k, \\mathcal{F}_k, \\mathcal{I}_k)$. \\Comment{Choose the worst agent}\n    \\State $\\mathcal{O}_{k^*} \\leftarrow \\mathcal{O}_{k^*}\\cup \\{\\mathcal{O}_{k^*}^T\\}$. \\Comment{Add new paper to output}\n    \\State $\\mathcal{I}_{k^*}\\leftarrow \\mathcal{I}_{k^*}\\cup \\{\\mathcal{O}_{k^*}^T\\}$. \\Comment{Add new paper to instructions}\n    \\State $\\mathcal{H}_{t} \\leftarrow \\mathcal{H}_{t-1} \\cup \\{\\mathcal{O}_{k^*}^T\\}$. \\Comment{Add to discovery history}\n    \\State $\\mathcal{F}_{k^*} \\leftarrow \\mathcal{F}_{k^*} \\cup \\{\\mathcal{O}_{k^*}^T\\}$. \\Comment{Add new paper to feedback}\n    \\State \\textbf{do} $\\mathcal{A}_k\\mathcal{G}(\\mathcal{I}_k, \\mathcal{H}_{t}, \\mathcal{F}_{t})$. \\Comment{Rerieve the worst agent}\n    \\State $\\mathcal{F}_k \\leftarrow \\mathcal{A}_k\\mathcal{G}(\\mathcal{I}_k, \\mathcal{H}_{t}, \\mathcal{F}_{t})$. \\Comment{Feedback from agents}\n    \n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n\\subsubsection{Evaluation Metrics}\nThe first step in evaluating AI-generated content is determining what counts as high-quality generation~\\citep{NEURIPS2023_91f18a12,reid2024gemini}. In this paper, we use both automated methods and human evaluations to assess the quality of generated outputs, with a primary focus on three dimensions:\n\n\\begin{itemize}\n    \\item \\textit{Relevance and Novelty}: Whether the agent's outputs contain novel and significant contributions with respect to existing literature. The literature includes recent and related papers within a specific topic. \n    \\item \\textit{Soundness}: Whether there are obvious and major logical or factual errors in the agent's outputs.\n    \\item \\textit{Coherency}: Whether the reasoning provided by the agent's outputs is logical and consistent with itself and the given project. This dimension assesses the overall quality of writing, including the use of technical terms, citations, and scientific knowledge.\n\\end{itemize}\n\nAll the above dimensions need to be evaluated collectively. In some cases, an output that is sound but not very novel or coherent can still be a good output. The three dimensions need to be evaluated as a whole to give a comprehensive assessment of AI-generated content~\\citep{nuijten2016prevalence}. Therefore, we use both human evaluation and AI judge~\\citep{NEURIPS2020_6b493230,robertson2023gpt4,li2024ai4r} as our evaluation method. \n\n\\subsection{Inconsistent Performance}\nIn this section, we conduct experiments to investigate \"inconsistent performance\" using the LLM-as-judge method~\\citep{NEURIPS2020_6b493230,robertson2023gpt4}. We use GPT-4 to simulate literature research, problem formulation, and mathematical derivation in the target tasks. We use GPT-4 as a judge to judge whether the outputs generated by our method are better than those generated by agents. We sample from a large number of different research fields, which can give us a more convincing result to reveal the potential of GPT-4 in assisting scientific discovery. The results can be found in Table~\\ref{tab:human_study_7}.\n\\begin{table}[h]\n\\centering\n\\caption{Researchers' opinions on whether GPT-4 can assist in scientific discovery.}\n\\label{tab:human_study_7}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c|c|c}\n\\hline\n\\multirow{2}{*}{Groups} & \\multicolumn{4}{c|}{Researchers} & \\multicolumn{4}{c}{Others} \\\\ \\cline{2-9} \n& Others & Neuroscience & NLP & Molecular Biology & Others & Machine Learning & Cell Biology & Average & Variance \\\\ \\hline\nResearcher 1 & 750 & 683 & 660 & 690 & 640 & 662 & 696 & 674.3 & 18.5 \\\\\nResearcher 2 & 569 & 606 & 608 & 647 & 665 & 675 & 703 & 637.1 & 25.9 \\\\\nMaster & 681 & 645 & 690 & 703 & 698 & 741 & 751 & 693.3 & 20.5 \\\\\nDoctor & 687 & 676 & 691 & 709 & 671 & 671 & 716 & 691.1 & 14.3 \\\\\nResearch Group & 691 & 665 & 651 & 707 & 650 & 698 & 751 & 690.9 & 32.8 \\\\ \\hline\n\\end{tabular}\n}\n\\end{table}\n\nSpecifically, \\textbf{Researcher 1} is a professor from CS; \\textbf{Researcher 2} is an associate professor from ML; \\textbf{Master} is a master student from CV; \\textbf{Doctor} is a Ph.D. student from ML; and \\textbf{Research Group} includes four papers generated by our agents. The group \\textbf{Others} is the remaining. The seven examinee groups have various abilities to reveal the potential of GPT-4 in assisting scientific discovery. The numbers at the bottom of the table represent the number of queries for each group. The table details the scores from different groups and their variance is relatively large. The results suggest that the performance of the models varies significantly across different research fields. Moreover, we sample 100 papers generated by our methods and ask researchers and beginners to score them. The scores from CS, ML, CV, NLP, and Biology can be regarded as researchers, while the scores from other topics are regarded as beginners. The results show that the models perform best in molecular biology, followed by machine learning and neuroscience. Although the consistency of the outputs is relatively stable, there is still a lack of consistency in different research fields. \n\nFurthermore, the results show that the model has a reasonable understanding of science, but the model's performance in the machine learning and neuroscience fields is less helpful compared to other domains such as molecular biology. This is consistent with the findings of \\cite{hayes2024simulating}, which suggest that the model's ability to assist in the machine learning and neuroscience domains is relatively weak. \n\n\\subsection{Automatic Discovery Process}\n\nIn this section, we conduct experiments to demonstrate that the \\textit{singular} approach of agents to directly produce a scientific discovery is not effective. We found that the outputs of language models have relatively high consistency when given the same prompt. To investigate the performance of this method, we let the language agents conduct the whole process autonomously. The overall quality and consistency of the outputs are tested by asking GPT-4 to judge whether the output of research claim number two is better than research claim number one. The results are shown in \\cref{fig:performance}. It can be seen that the overall quality and consistency of the outputs are low and unstable. \n\n\\begin{wrapfigure}[16]{r}{0.4\\textwidth}\n    \\centering\n    \\includegraphics[width = 0.38\\linewidth]{original.pdf}\n    \\caption{Consistency and Validity}\n    \\label{fig:performance}\n\\end{wrapfigure}\n\n\\subsection{Autonomous Agents}\nAs shown in Figure~\\ref{fig:overview}, the autonomous agents can potentially lead to higher efficiency within the field of science, independently generating and and refining scientific discoveries. These agents are not designed to have the overall objective, e.g., novel contribution, and work independently. Mathematical derivation provides a deeper insight into data and phenomenon by translating them into mathematical language, this process helps to formalize research objectives, unify observations, and formalize data into something more consistent. It also helps to find errors and find different ways to achieve research objectives. \n\\begin{table}[h]\n\\centering\n\\caption{Performance of Autonomous Agents.}\n\\label{tab:agents}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|c|c|c|c}\n\\hline\n\\multicolumn{1}{l|}{Agents} & Start & Finish & Total Time & Cost \\\\ \\hline\nGPT-4 & 2023-09-23 11:21 & 2023-11-19 16:21 & 2 months & \\$ 7,410 \\\\ \\hline\nQwen2.5-14b & 2023-11-19 17:21 & 2023-12-23 09:21 & 2 weeks & \\$325 \\\\ \\hline\nLLaMA3.1-8b & 2023-11-19 11:21 & 2023-12-09 16:21 & 2 weeks & \\$185 \\\\ \\hline\n\\end{tabular}\n}\n\\end{table}\n\n\\begin{table}[]\n\\caption{Automatic Agents vs Human.}\n\\label{tab:agent_vs_human}\n\\centering\n\\resizebox{0.6\\linewidth}{!}{\n\\begin{tabular}{l|c}\n\\hline\nUsers & Avg. Score \\\\ \\hline\nResearchers & 5.39 \\\\ \\hline\nBeginners & 4.75 \\\\ \\hline\n\\end{tabular}\n}\n\\end{table}\n\n\\begin{table}[]\n\\caption{Automatic Agents vs Bot.}\n\\label{tab:agent_vs_bot}\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|l|l|l}\n\\hline\nTopics & Method & Score & \\#x \\\\ \\hline\n\\multirow{3}{*}{\\makecell[l]{Vision\\&Imaging}} & GPT-4 & 6.21 & 54.9 \\\\\n & Qwen2.5 & 6.46 & 5 \\\\ \n & AIScientist & 7.12 & P \\\\ \\hline\n\\multirow{2}{*}{NLP} & GPT-4 & 6.58 & 50 \\\\\n & Qwen2.5 & 6.89 & 48.3 \\\\ \\hline\n\\multirow{2}{*}{Language\\&Speech} & GPT-4 & 6.58 & 50 \\\\\n & Qwen2.5 & 6.16 & 40.4 \\\\ \\hline\n\\multirow{2}{*}{Machine Learning} & GPT-4 & 6.18 & 51.7 \\\\\n & Qwen2.5 & 5.99 & 35.1 \\\\ \\hline\n\\end{tabular}\n}\n\\end{table}\n\n\\subsection{Results}\\label{sec:results}\nIn this section, we conduct experiments to demonstrate the performance of autonomous agents. We apply ScientificSniper to 21 topics, collecting a total of 152 project descriptions (each project may have several descriptions) and using them as inputs to the system. The average scores are shown in \\cref{tab:agents}. It can be found that the results suggest that we can achieve a comparable level of output quality with that of a human. In addition, as shown in \\cref{tab:agents}, a larger model can produce better, more reasonable results, but this does not mean that smaller models cannot be used to write good papers. As shown in \\cref{tab:agents}, we have deployed two models, including a smaller LLaMA3.1-8b model and a larger GPT-4 model. The LLaMA3.1-8b model, though smaller in size, can produce good results with a higher efficiency. The GPT-4 model, however, while demonstrating exceptional performance in both quality and efficiency, incurs significantly higher financial costs. These findings highlight the trade-offs between model size, performance, and resource utilization in the context of scientific discovery. Our approach suggests that it is possible to achieve close to state-of-the-art results without deploying large-scale models, thereby offering a more efficient and cost-effective solution.\n\nIn addition, we conducted an extensive experiment to demonstrate that our system is generally regarded as \"\\textit{not bad}\" by both researchers and beginners. We have evaluated the papers by 100 users and the user scores are shown in \\cref{tab:agent_vs_human}. Among them, 20 are researchers in CS, ML, CV, NLP, and Biology, while the remaining 80 are beginners. The user can score the paper from 1 to 10. The results show that the average score of researchers is higher than that of beginners, suggesting that our papers are more accepted by experts in the field, although they have some deficiencies. In contrast, if the outputs are generated by GPT-4 directly, our system will have a similar score as GPT-4, but higher than GPT-4 without project description as input.\n\nThe system can achieve a relatively high level of performance in some specific topics. We conducted experiments to demonstrate that our system can assist researchers with writing scientific papers. We use AI4Science~\\citep{ai4science2023impact} to write a research paper against topics as the baseline method. The topics are selected from GPT-4 and integrate potential research directions in the future. We apply both GPT-4 and Qwen2.5 to apply this method. In addition, we apply our methods to the same topic and compare the results. The results are shown in \\cref{tab:agent_vs_bot}. It can be found that, compared with the method of randomly picking a topic and directly asking GPT-4 to write a paper, our system can achieve a higher score as well as more detailed content.\n\n\n\n\\section{Conclusion}\n\nThis paper presents ScientificSniper, a collaborative multi-agent system designed to emulate the incremental processes of scientific discovery. It includes a team of six agents, each specialized in different aspects of the research process. Together, they have successfully automated the entire process of a multi-disciplinary discovery task, providing a high-level abstraction of literature, research problem, mathematical derivation and validation, with each discovery iteratively backed by theoretical analysis, and used effectively in re-training the agents with a greater amount of data. These agents work independently and are not equipped with an overall objective, e.g., novel contribution, and work independently. Mathematical derivation provides a deeper insight into data and phenomenon, helping to formalize research objectives, unify observations, and formalize data into something more consistent. It also helps to find errors and find different ways to achieve research objectives.\nFor future research, we may further generalize the system to be more autonomous by automatically selecting topics, as well as agentically reviewing the papers~\\citep{tan2024peerreviewmultiturnlongcontext,tyser2024ai,d2024marg,baek2024researchagent}. In addition, we may further",
  "motivation": "\n\nThe field of artificial intelligence (AI) has long explored the development of AI agents capable of scientific discovery. While existing methods have shown promise, they often treat scientific discovery as a singular task, neglecting the intrinsic processes that constitute science as a multifaceted domain. This approach includes two primary shortcomings: first, it ignores the specialized expertise required by specific research fields, leading to inconsistent performance across various scientific topics. Second, it overlooks the intricate processes involved in scientific discovery, such as literature summarization, research problem formulation, experimental designing, discovery execution, and validation, resulting in a lack of generalizability. This paper addresses these limitations by introducing ScientificSniper, a collaborative multi-agent system tailored to emulate the processes of scientific discovery. ScientificSniper consists of six specialized agents, each adept at its corresponding skill. Leveraging a data-driven training strategy, ScientificSnier has successfully automated the entire process of a multi-disciplinary discovery task, providing a high-level abstraction of literature research, problem formulation, mathematical derivation, and validation, with each discovery iteratively backed by theoretical analysis, used effectively in re-training the agents with a greater amount of data. The system can autonomously produce a research paper with solid quality, which is comparable to that of humans, in a short period of time. The paper aims to demonstrate that the collaboration of multiple agents can potentially lead to higher efficiency within the field of science, independently generating and and refining scientific discoveries, ultimately enhancing the research process.\n\n",
  "idea": "\n\nThe paper presents ScientificSniper, a collaborative multi-agent system designed to emulate the incremental processes of scientific discovery. This system consists of specialized AI agents that work together to overcome the limitations of singular approaches, such as inconsistent performance across research fields and the neglect of intricate process involved in scientific discovery. The AI agents are trained to be capable of literature research, problem formulation, mathematical derivation, and validation. The system demonstrates the potential of leveraging artificial intelligence to autonomously generate and refine scientific discoveries, ultimately enhancing the research process.\n\n",
  "interestingness": "\n\n8\n\n",
  "feasibility": "\n\n9\n\n",
  "novelty": "\n\n7\n\n",
  "title": "Autonomous Machine Learning Research with AI Agents\n\n",
  "abstract": "\nThe field of artificial intelligence (AI) has long been devoted to developing AI agents that are capable of scientific discovery. However, this article reveals two limitations in the prevalent methods. First, these methods are designed to mimic a singular ability that can directly produce a scientific discovery, ignoring the specialized expertise required by the specific research fields, resulting in inconsistent performance across diverse scientific topics. Second, they treat scientific discovery as a singular task, overlooking the intricate processes that constitute science as a multifaceted domain, resulting in a lack of generalizability. This paper presents ScientificSniper, a collaborative multi-agent system tailored to emulate the \\textit{processes} of scientific discovery. This system includes a team of six specialized agents, with each agent adept at its corresponding skill. Leveraging a data-driven training strategy, ScientificSniper has successfully automated the entire process of a multi-disciplinary discovery task, providing a high-level abstraction of literature research, problem formulation, mathematical derivation, and validation, with each discovery iteratively backed by theoretical analysis, used effectively in re-training the agents with a greater amount of data. The system can autonomously produce a research paper with solid quality, which is comparable to that of humans, in a short period of time.\n",
  "Experimental_Setup": "\n[{\"name\": \"Inconsistent Performance\", \"description\": \"This experiment aims to investigate the inconsistent performance of language models across different research fields. The paper uses the LLM-as-judge method to simulate literature research, problem formulation, and mathematical derivation in the target tasks. The GPT-4 judge is used to compare the outputs generated by the paper with those generated by the agents. The evaluation is conducted on seven examinee groups with various abilities to reveal the potential of GPT-4 in assisting scientific discovery. The results show that GPT-4 has a reasonable understanding of science, but the assistance is more effective in some specific research fields such as molecular biology and less helpful in machine learning and neuroscience.\"}, {\"name\": \"Automatic Discovery Process\", \"description\": \"This experiment aims to demonstrate that the singular approach of agents to produce a scientific discovery is not effective. The paper found that the outputs of language models have relatively high consistency when given the same prompt multiple times. To investigate the performance of this method, the paper lets the language agents conduct the whole process autonomously. The overall quality and consistency of the outputs are tested by asking GPT-4 to judge whether the output of research claim number two is better than research claim number one. The results show that the overall quality and consistency of the outputs are low and unstable.\"}, {\"name\": \"Autonomous Agents\", \"description\": \"This experiment aims to demonstrate that the collaboration of multiple agents can potentially lead to higher efficiency within the field of science, independently generating and and refining scientific discoveries. The paper presents ScientificSniper, a multi-agent system that includes six agents, each with a specific ability to abstract the research problem, retrieve and summarize related literature, formulate novel ideas, and formalize these ideas through mathematical derivation, and validation. The agents work independently and are not equipped with the overall objective, e.g., novel contribution.\"}, {\"name\": \"Autonomous Agents Performance\", \"description\": \"This experiment aims to demonstrate the performance of autonomous agents. The paper applies ScientificSniper to 21 topics, collecting a total of 152 project descriptions (each project may have multiple descriptions) and using them as inputs to the system. The average scores are shown in the table. The paper compares the outputs of the system with top papers from previous years in each topic. The results suggest that the system can achieve a comparable level of output quality as that of a human.\"}, {\"name\": \"Automatic Agents Against Human\", \"description\": \"This experiment aims to demonstrate that the paper's system output is generally recognized as 'not bad' by both researchers and beginners. The paper has evaluated the papers by 100 users and the user scores are shown in the table. Among them, 20 are researchers in CS, ML, CV, NLP, and Biology, while the remaining 80 are beginners. The user can score the paper from 1 to 10. The results show that the average score of researchers is higher than that of beginners, suggesting that the papers are more accepted by experts in the field, although they have some deficiencies.\"}, {\"name\": \"Automatic Agents Against Bot\", \"description\": \"This experiment aims to demonstrate that the system can achieve a relatively high level of performance in some specific topics. The paper uses AI4Science to write a research paper against topics as the baseline method. The topics are selected from GPT-4 and integrate potential research directions in the future. The paper applies both GPT-4 and Qwen2.5 to write a single research paper. The results show that the paper can achieve a higher score as well as more detailed content than the method of randomly picking a topic and directly asking GPT-4 to write a paper.\"}]\n\n",
  "Experimental_results": "\n[{\"name\": \"Inconsistent Performance\", \"result\": {\"table\": [[\"Groups\", \"Others\", \"Neuroscience\", \"NLP\", \"Molecular Biology\", \"Machine Learning\", \"Cell Biology\", \"Organism Biology\", \"Average\", \"Variance\"], [\"Researcher 1\", 750, 683, 660, 690, 640, 662, 696, 674.3, 18.5], [\"Researcher 2\", 569, 606, 608, 647, 665, 675, 703, 637.1, 25.9], [\"Master\", 681, 645, 690, 703, 679, 698, 741, 693.3, 20.5], [\"Doctor\", 687, 676, 691, 709, 673, 671, 716, 691.1, 14.3], [\"Research Group\", 691, 665, 651, 707, 650, 698, 751, 690.9, 32.8], [\"Beginners\", 627, 651, 625, 608, 616, 620, 691, 637.1, 30.3]], \"notes\": [\"The table shows the scores from different groups and their variance is relatively large. The results suggest that the performance of the models varies significantly across different research fields. Moreover, the results show that the model has a reasonable understanding of science, but the model's performance in the machine learning and neuroscience fields is less helpful compared to other domains such as molecular biology.\"]}}, {\"name\": \"Automatic Discovery Process\", \"result\": {\"image\": \"Figure 2: Consistency and Validity.\", \"description\": \"The figure shows the consistency and validity of the agents' outputs. The overall quality and consistency of the outputs are low and unstable. Although the consistency of the outputs is relatively stable, there is still a lack of consistency in different research fields.\"}, {\"name\": \"Autonomous Agents\", \"result\": {\"image\": \"The overview of the ScientificSniper.\", \"description\": \"The figure provides an overview of the ScientificSniper, a collaborative multi-agent system that includes six agents, each with a specific ability to abstract the research problem, retrieve and summarize related literature, formulate novel ideas, and formalize these ideas through mathematical derivation, and validation. The agents work independently and are not equipped with the overall objective, e.g., novel contribution, and work independently.\"}, {\"name\": \"Autonomous Agents Performance\", \"result\": {\"table\": [[\"Agents\", \"Start\", \"Finish\", \"Total Time\", \"Cost\", \"Avg. Score\"], [\"GPT-4\", \"2023-09-23 11:21\", \"2023-11-19 16:21\", \"2 months\", \"$ 7,410\", 5.81], [\"Qwen2.5-14b\", \"2023-11-19 17:21\", \"2023-12-23 09:21\", \"2 weeks\", \"$325\", 4.96], [\"LLaMA3.1-8b\", \"2023-11-19 11:21\", \"2023-12-09 16:21\", \"2 weeks\", \"$185\", 4.57]], \"notes\": [\"The table shows the performance of different agents in terms of start and finish time, total time, cost, and average score. The results suggest that the system can achieve a comparable level of output quality as that of a human.\"]}}, {\"name\": \"Automatic Agents Against Human\", \"result\": {\"table\": [[\"User\", \"Avg. Score\"], [\"Researchers\", 5.39], [\"Beginners\", 4.75]], \"notes\": [\"The table shows the average scores from researchers and beginners. The results suggest that the papers are more accepted by experts in the field, although they have some deficiencies.\"]}}, {\"name\": \"Automatic Agents Against Bot\", \"result\": {\"table\": [[\"Topics\", \"Method\", \"Score\", \"$x$\"], [\"Vision & Imaging\", \"GPT-4\", 6.21, 54.9], [\"Vision & Imaging\", \"Qwen2.5\", 6.46, 5], [\"Vision & Imaging\", \"AIScientist\", 7.12, \"P\"], [\"NLP\", \"GPT-4\", 6.58, 50], [\"NLP\", \"Qwen2.5\", 6.89, 48.3], [\"Language & Speech\", \"GPT-4\", 6.58, 50], [\"Machine Learning\", \"GPT-4\", 6.18, 51.7], [\"Machine Learning\", \"Qwen2.5\", 5.99, 35.1]], \"notes\": [\"The table shows the performance of the system against AIScientient in different topics. The results suggest that the system can achieve a higher score as well as more detailed content than the method of randomly picking a topic and directly asking GPT-4 to write a paper.\"]}}]",
  "latex": "\n\\title{Autonomous Machine Learning Research with AI Agents}\n\n\\begin{abstract}\nThe field of artificial intelligence (AI) has long been devoted to developing AI agents that are capable of scientific discovery. However, this article reveals two limitations in the prevalent methods. First, these methods are designed to mimic a singular ability that can directly produce a scientific discovery, ignoring the specialized expertise required by the specific research fields, resulting in inconsistent performance across diverse scientific topics. Second, they treat scientific discovery as a singular task, overlooking the intricate processes that constitute science as a multifaceted domain, resulting in a lack of generalizability. This paper presents ScientificSniper, a collaborative multi-agent system tailored to emulate the \\textit{processes} of scientific discovery. This system includes a team of six specialized agents, with each agent adept at its corresponding skill. Leveraging a data-driven training strategy, ScientificSniper has successfully automated the entire process of a multi-disciplinary discovery task, providing a high-level abstraction of literature research, problem formulation, mathematical derivation, and validation, with each discovery iteratively backed by theoretical analysis, used effectively in re-training the agents with a greater amount of data. The system can autonomously produce a research paper with solid quality, which is comparable to that of humans, in a short period of time.\n\\end{abstract}\n\n\\section{Introduction}\n\n\n\n\nOver the past decade, large language models (LLMs) have made significant strides in various domains~\\citep{achiam2023gpt, reid2024gemini, qwen2.5}. These models exhibit remarkable proficiency in planning, reasoning, and interactive capabilities, showcasing their potential to drive the advancement of knowledge. Researchers have started to explore whether AI can assist or entirely autonomously drive scientific discovery~\\citep{wang2023scientific}, an area that has traditionally been the domain of human expertise and critical thinking. Among these, AutoAlchemy \\citep{clune2023automated} and AgentReview \\citep{jin2024agentreivewexploringpeerreview} are among the most closely related works to ours. This paper extends this research trajectory by investigating the potential of AI agents to function autonomously as researchers, identifying research topics, researching relevant literature, formulating findings, and presenting these ideas in technical papers. The entire process is achieved without human intervention.\nOur method has discovered and submitted 21 machine-generated scientific papers on arXiv, which have received Viewing counts as high as \\textit{10,000 times}. \n\nSpecifically, this paper reveals two limitations in the existing methods: First, the singular research ability may result in inconsistent performance. The singular research ability refers to a language-based method that has the capability to directly produce scientific discoveries~\\citep{clune2023automated}. \n\\citet{hayes2024simulating} conducted a large-scale pilot study on 1008-question datasets from seven examinee groups with various abilities to reveal the potential of GPT-4 in assisting scientific discovery. The results show that GPT-4 has a reasonable understanding of science, but the assistance is more effective in some specific research fields such as molecular biology and less helpful in machine learning and neuroscience. The inconsistent performance is primarily due to the varied requirements of different research topics, some of which rely more on literature research and conceptual innovation, while others require a greater degree of domain-specific expertise. Therefore, rather than focusing on a singular ability to directly produce discoveries, the paper introduces ScientificSniper, a multi-agent system consisting of six agents, each with a specific ability to abstract the research problem, retrieve and summarizing related literature, formulating novel ideas, and formalizing these ideas through mathematical derivation. The results show that this approach significantly outperforms the singular research ability. \n\nSecond, the prevalent research methods treat scientific discovery as a singular task, overlooking the intricate process involved in scientific discovery~\\citep{langley1987scientific,clune2023automated}. Conceptualizing the scientific discovery process as a singular task in LLMs may simplify the research process~\\citep{hayes2024simulating,clune2023automated}. However, this approach overlooks the subtle and nuanced understanding that humans acquire through engagement with various aspects of discoveries such as literature research, problem formulation, mathematical derivation, and rigorous validation. Consequently, this can lead to a lack of adaptability and generalizability in different discovery contexts. To address this issue, the paper presents ScientificSniper, a multi-agent system that emulates the incremental processes of scientific discovery. Specifically, it includes two research assistants that are responsible for research assistance and technical writing. These two assistants work together to provide a high-level abstraction of the research process, with each discovery being systematically backed by theoretical analysis, used effectively in re-training the agents with a greater amount of data, thereby achieving exponential improvement. \n\nThe contributions of this paper are summarized as follows:\n\\begin{itemize}\n    \\item \n    This paper reveals two limitations in the singular research approaches: First, the singular research ability may result in inconsistent performance across various research topics. Second, the singular research approach overlooks the intricate process involved in scientific discovery.\n    \\item \n    This paper presents ScientificSniper, which consists of six agents, each with a specific ability to abstract the research problem, retrieve and summarizing related literature, formulating novel ideas, and formalising these ideas through mathematical derivation, and validation.\n    \\item \n    This paper demonstrates that the collaboration of multiple agents can potentially lead to higher efficiency within the field of science, independently generating and and refining scientific discoveries, ultimately enhancing the research process.\n\\end{itemize}\n\n\n\n\\section{Related Works}\n\n\n\\subsection{Large Language Models for Scientific Discovery}\nA series of research has explored the potential of artificial intelligence, particularly large language models (LLMs), to function autonomously as a scientific researcher~\\citep{clune2023automated,huang2024mlagentbench,li2024ai4r,yang2024collaborative}. The research activities encompass literature research, problem formulation, experimental designing, and presentation of technical papers. AI researchers have undertaken a large-scale pilot study to examine the potential of GPT-4 in assisting scientific discovery~\\citep{hayes2024simulating}. The experimental results indicate that the GPT-4 can effectively answer questions in biology and chemistry; however, the assistance is less effective for physics and machine learning. \\citet{ai4science2023impact} have explored the impact of AI on natural language processing by conducting a controlled experiment using GPT-4 to speed up the process of research and paper writing. The results indicate that the application of AI in NLP (natural processing language) can potentially accelerate the progression of research while also posing ethical challenges. \\citet{merchant2023scaling} and \\citet{pyzer2022accelerating} have explored the potential of deep reinforcement learning and artificial intelligence to accelerate material discovery with significantly improved accuracy and efficiency. \\citet{jumper2021highly} have developed AlphaFold, a method for predicting the three-dimensional structure of protein molecules from amino acid sequences, this technology has garnered significant attention due to its potential implications for the fields of bioinformatics and computational biology. It can be applied to tasks such as protein folding prediction, protein design, and protein structure modelling. \\citet{buchanan1981dendral} have developed a system that aims to detect and recognize chemical compounds based on mass spectra data. This system is designed to be flexible and adaptable, accommodating different analytical techniques. \\citet{langley2024integrated} have discussed a class of AI systems designed to automatically generate, validate, and adjust scientific knowledge. They have worked on automatically correcting or completing incomplete models of domains such as physics and chemistry. \\citet{wang2023scimon} have explored computational approaches to accelerate the process of scientific discovery in fields such as drug design and materials science. They have proposed a method to automatically generate testable claims and hypotheses, focusing on the development of new algorithms and representation techniques for managing the complexity inherent in scientific discovery. \\citet{hayes2024simulating} and \\citet{romera2024mathematical} have proved the concept that the scientific discovery process can be emulated by searching for mathematical expressions to produce discoverable content such as scientific paper. \\citet{wang2024autosurvey} have explored the potential of AI-powered systems to automate the creation, development, and implementation of scientific surveys. The proposed methodology aims to automate the process of designing and distributing surveys to large pools of participants, analyzing the collected data to automatically draw reasonable conclusions. \\citet{langley1987scientific} have studied the use of computational techniques in fostering the creation of new knowledge in specific domains. The study aims to stimulate, accelerate, and direct the creation of new ideas to generate new knowledge in given domains. Many works have studied large language models to assist scientific research~\\citep{liang2024mapping}. \nSpecifically, \\citet{du2024llms} have examined the use of large language models to automatically critique paper. \\citet{liu2024towards} have explored the possibility of using large language models to accelerate the process of scientific research in the field of materials science. \\citet{wang2023scientific} have demonstrated the potential of using large language models to accelerate the speed and efficiency of scientific discovery in a specific domain. \\citet{yakaboski2023ai} have focused on the process of creating new knowledge in specific domains through the use of artificial intelligence and machine learning techniques. \\citet{langley2024integrated} has presented a class of AI systems designed to support the creation of new knowledge in given domains. \\citet{li2024ai4r} have studied the use of artificial intelligence to assist in accelerating the creation of new knowledge in given domains. \\citet{liang2024can} have shown the potential of using AI to assist with scientific research tasks.\n\n\\subsection{Large Language Models for Natural Language Processing}\nA series of research has explored the potential of LLMs, which have demonstrated remarkable proficiency in planning, reasoning, and interactive capabilities, to assist in natural language processing~\\citep{NEURIPS2020_6b493230}. AutoAlchemy~\\citep{clune2023automated} has pioneered the exploration of automatic research. By leveraging an AI assistant trained on large-scale datasets of scientific papers, it is possible to generate questions, create research objectives, and further elaborate these objectives into detailed project descriptions and rigorous proofs. The research process can then be streamlined by using feedback to formative papers to re-train the AI assistant. \\citet{huang2024mlagentbench} have studied the effectiveness of language agents in machine learning experiments, showcasing their proficiency in tasks such as experiment design, model selection, and optimization. \\citet{jin2024agentreivewexploringpeerreview} have studied the feasibility of language agents in the peer review process. The experiments have shown that language agents are capable of providing constructive reviews, offering detailed and technical feedback on key aspects of papers. \\citet{hu2024automated} have studied the possibility of using language agents to assist with the development and implementation of agentic systems. \\citet{bao2021predicting} have proposed a method to predict the expected success of papers at the NeurIPS conference based on the quality of their reviews. \\citet{wei2022chain} have studied how to enhance the reasoning ability of language models by prompting it to articulate a thought process step by step. \\citet{weng-etal-2023-large} have studied the possibility of using language agents to assist with the development and implementation of agentic systems. \\citet{shinn2023reflexion} have proposed a novel framework for exploring the potential of AI agents as research assistants in the field of natural language processing. The experiments have demonstrated a notable enhancement in the ability to comprehend and interpret complex natural language text. \\citet{zhang2024generative} have studied the possibility of using language agents to assist with the development and implementation of agentic systems. The experiments have demonstrated a remarkable ability to comprehend and interpret complex natural language text, leading to a significant improvement in performance.\n\n\\subsection{Reward Model and Preference Optimization}\nA series of research has studied the training of reward models to assist in reinforcement learning.\nA notable work in this area is the study of Iterative Rank Choice (IRC)~\\citep{rafailov2023direct}. This technique successfully harnesses language models to serve as effective reward models. IRC involves an iterative process that incrementally trains a reward model using feedback from human users, selecting the most preferred options in each iteration. This approach consistently improve the performance of the reward model over time, aligning more closely with the preferences of human users. \\citet{xu2023wraprankmodel} have proposed the Reinforcement Learning from Human Feedback (RLHF) process in which a reward model is learned from human feedback. Iterative Rank Choice (IRC) refers to the process of training a reward model by iteratively collecting and utilizing human preferences. In each iteration, the model predicts preferences based on initial data and collects new feedback for fine-tuning. The process is repeated to improve the model's performance. Iterative preference optimization (IPO) is similar to IRC, but it uses active choice pairs to generate new feedback for each iteration. In each iteration, pairs of data are selected to maximize the expected utility of generating new feedback. SimPO~\\citep{meng2024simposimplepreferenceoptimization} has introduced a new algorithm that does not require a reference, in contrast to IPO. The algorithm learns a reward model from human feedback data by ranking different options, similar to IPO. The proposed algorithm does not require a reference, which is a known value, and instead learns a policy that maximizes the expected reward with human feedback. \\citet{xiong2024iterativepreferencelearninghuman} have studied the problem of optimizing language models for reinforcement learning from human feedback (RLHF) using iterative preference optimization.\nThe proposed approach is based on the assumption that humans make decisions in a contextual manner, considering the context of a given situation when making choices between options. The algorithmic objective of the proposed approach is to learn a language model that can make decisions based on the context, aligning with human preferences.\n\\begin{figure}\n\\centering\n    \\includegraphics[width = \\linewidth]{overview.pdf}\n    \\caption{Overview of the ScientificSniper.}\n\\label{fig:overview}\n\\end{figure}\n\n\n\\section{Methods}\n\n\n\\subsection{Overview}\nThis section gives an overview of the ScientificSniper. As shown in Figure~\\ref{fig:overview}, it is a collaborative multi-agent system that consists of six autonomous agents that can emulate the processes of scientific discovery. Each agent has a specific ability to abstract the research problem, retrieve and summarise related literature, formulating novel ideas, and formalising these ideas through mathematical derivation, and validation. These agents are not equipped with the overall objective, e.g., novel contribution, and work independently.\n\n\\subsection{Autonomous Agents}\nAutonomous Agents can refer to any AI agent that is capable of autonomously generating output without human intervention. This paper employs GPT-4~\\citep{achiam2023gpt} or Qwen2.5-14b~\\citep{qwen2.5} to develop various autonomous agents. In this system, each agent is trained with the text generation task using the standard supervised finetuning protocol in HuggingfaceTrainedText-to-TextLanguageModel~\\citep{Bengio+chapter2007,Hinton06}. These agents are not designed to have the overall objective, e.g., novel contribution, and work independently.\n\n\\subsubsection{Researcher Assistant}\nResearcher Assistant agent works as a research assistant for a team of six agents. Its main purpose is to articulate a clear research claim based on the given paper information. It takes two key components as context to clarify research claims: (1) \\textit{Paper Information} refers to e.g., the address, phone and email number of the corresponding authors, name of the institutions, publication name and so on. (2) \\textit{Literature Review} is summarized from e.g., literature research; the literature research can be conducted by Literature Agent. Specifically, it will study relevant literature (by retrieving literature from arXiv based on a few related keywords) and write a research assistant. Based on these, it will propose a research claim. Besides, it will review the latest literature to ensure that the research claim is novel to existing literature. It will also provide theoretical analysis to support the claim. To better understand the literature, it may ask Literature Research Agent to summarize the literature. To provide theoretical analysis, it may ask Math Agent to derive formulas and ask Reasoner Agent to check for problems in them. After that, it will write a formal paper to describe the core concept and its significance for machine learning research, computer vision research, biology science research, and so on.\n\\subsubsection{Literature Research Agent}\nLiterature Research Agent can refer to any language agent that is capable of researching literature in a given field. Its main purpose is to provide relevant and helpful literature for the rest of the agents. It will first conduct a literature research via searching literature databases (e.g. arXiv, Google Scholar) with keywords extracted from the given project description. After that, it will write a research assistant based on the retrieved literature. Specifically, it will retrieve relevant literature by conducting a search on literature databases such as Google Scholar, arXiv, and ResearchGate using the keywords extracted from the provided project description. The retrieved literature will include technical articles, conference papers, and technical reports relevant to the topic. The literature will be summarized by the Literature Agent according to the specific requirements of the project. The literature summary will include a summary of the key findings, related literature, related fields, or other information. The specific content of the literature summary will be based on the retrieved literature and the project's requirements.\n\n\\subsubsection{Literature Summarize Agent}\nLiterature Summarize Agent can refer to any language agent that is capable of writing a summary based on a given literature. It takes literature as input and will write a summarized literature based on it. Specifically, it will write a summarized literature based on the retrieved literature from Literature Research Agent. Such literature-based summaries are used to improve the research of other agents. A literature-based summary is a type of writing that summarizes the main points, key ideas, or important information of a literature piece. It aims to provide a concise and clear summary of the main points, background, methods, results, and conclusions of the literature, helping readers quickly understand the key content. Literature Summarize Agent is important in a research project because it can save significant time and effort in reviewing and understanding the main points of a large amount of literature. For example, literature summarize agent can provide a concise and clear summary of a long literature piece. This can help project members quickly understand the core content, saving the time to read the entire literature.\n\\subsubsection{Problem Formulation Agent}\nProblem Formulation Agent can refer to any language agent that is capable of formulating a research problem. Its main purpose is to formulate a research problem based on the given project description. It will first study the project information (that is, literature research and summarized literature from Literature Research Agent and Literature Summarize Agent), and then it will formulate a problem based on that. Specifically, it will conduct a detailed analysis and interpretation of the project description, to identify the key challenges and constraints associated with the project. It will then use this information to develop a formal problem statement. The problem statement will include a clear description of the project's objective, key constraints and assumptions, and the criteria for success. By formalizing the problem, it will provide a structured and precise framework for understanding and solving the key challenges.\n\n\\subsubsection{Math Agent}\nMath Agent can refer to any language agent that is capable of writing formulas. Its main purpose is to provide formulas to formulate a complex research problem based on the given project description. It will take a specific project description as input as well as literature research and literature summarization to produce formulas. Specifically, it will use literature research (from Literature Research Agent) and summarized literature (from Literature Summarize Agent) to enhance its ability to generate relevant formulas and enhance the accuracy of formula generation. For example, if the project involves the field of natural language processing (NLP), the agent may generate formulas for computational models, linguistic rules, and statistical models used in the relevant research and development. In addition to generating relevant formulas, the agent may also iterate on and refine these formulas to better match the given project description, thereby enhancing the accuracy of the generated formulas.\n\n\\subsubsection{Validation Agent}\n\nValidation Agent can refer to any language agent that is capable of validating complex research. Its main purpose is to validate the derived formulas from Math Agent and check for problems. It will take three key components as context: (1) \\textit{Project Description} refers to the input specific project description. (2) \\textit{Math Formula} is the formula generated by Math Agent. (3) \\textit{Derivation Reasoning} refers to the process of reasoning how to get the formula as well as intermediate results used in the formulas (e.g., finite element methods, mesh generation techniques, geometric models). Specifically, it will first validate the given formula's physical meaning with the project description and literature research to ensure that the formula can be used to describe the key aspects of the project. After that, it will check whether there are some simple logic or calculation errors in the formula with intermediate reasoning. In addition to validation, it will provide detailed instructions on how to improve the formula. If multiple formulas are available, it will select the most feasible ones. If there are problems with formula generation (e.g., no valid results, infinite loop calculation, unreasonable parameters), it will ask Math Agent to fix it. After several rounds of iterations, it will output a placement of the best mathematical formula results.\n\\subsection{Reward Model and Preference Optimization}\nTo further enhance the capabilities of autonomous agents~\\footnote{Autonomous agents can be selected from GPT-4~\\citep{achiam2023gpt}, Qwen2.5~\\citep{qwen2.5}, and Mistral~\\citep{jiang2023mistral}.} in writing, we have fine-tuned reward models for these agents. The reward models are used to provide feedback to the language agents during training, helping to optimize their output quality. We have trained reward models using preference data collected from evaluations. We use Chain-of-Thought (Co-Tail) prompting~\\citep{wei2022chain}, asking AI agents to think step by step when producing outputs. We finetune the reward models on the preference data using supervised fine-tuning to provide feedback. We use reinforcement learning with human feedback (RLHF) approach~\\citep{achiam2023gpt} to collect preference data, as well as iterative preference optimization (IPO)~\\citep{rafailov2023direct} and active choice pairs to generate new feedback for each iteration. The specific process can be referred to~\\citet{rafailov2023direct} and \\citet{pang2024iterative}.\n\n\\subsection{Model Architecture}\nIn the implementation, we choose two backbone models: Mistral~\\citep{jiang2023mistral} and Qwen2.5~\\citep{qwen2.5}. Specifically, we use 200 million reserved tokens and the Mistral-7B model~\\citep{jiang2023mistral} as optimization objectives. The Mistral agent is based on the Mistral language model~\\citep{jiang2023mistral}. The specific prompt for the agent can be found in \\cref{app:prompt}. In addition, we have trained of two reward models, namely mistral-reward and qwen2.5-reward, for these two models, respectively.\n\n\\subsection{Implementation and Model Throughputput Details}\nIn addition to the above details, we also show the concrete outputs of agents in~\\cref{app:output} and the details of the deployment are shown in~\\cref{app:deploy}. In total, the agents submitted 96 projects to ICLR, NeurIPS, ICML, and AAAI. The projects cover topics in Machine Learning (in five categories: Machine Learning, Computer Vision, Natural Language Processing, Biology Science and Other), and it can be found in~\\cref{tab:projects}. One project will have seven files: Project Description (e.g. .md), Camera Ready Paper (e.g. .pdf), Paper Details (e.g. .json), Review (e.g. .json), Review Feedback (e.g. .json), Newly Generated Files (zip), and Agents (json). In the Newly Generated Files Zip File, it contains the following files: System Log (sys-log), System Output (sys-output), Agent Output (agent-output) and Error Log (err-log). Sys-log details the important operation information of the system. Sys-output is the output generated by agents, including action, request, and response between agents. Agent-output contains the response of agents. Err-log contains error information such as error type and a complete stack trace in case an error occurs in the system. In the Agents JSON file, it contains the role (e.g. Qwen2.5), temperature, role properties and etc. The results are all put on \\url{https://anonymous.4open.science/r/autonomous-ml-research-2F745A/}.\n\n\\begin{table}[]\n\\caption{Projects Covering Topics and Categories.}\n\\label{tab:projects}\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|l|c|ccc}\n\\hline\n\\textbf{Topics} & \\textbf{Categories} & \\textbf{Conf. & \\textbf{\\#} & \\textbf{\\#Top.} & \\textbf{\\#Bot.} \\\\ \\hline\nMachine Learning & Optimization & 2 & 2 & 1 & 1 \\\\\nMachine Learning & Agent & 2 & 2 & 1 & 1 \\\\\nMachine Learning & Bandit & 2 & 2 & 1 & 1 \\\\\nComputer Vision & Algorithm & 1 & 6 & 3 & 3 \\\\\nNatural Language Processing & Summarization & 1 & 4 & 2 & 2 \\\\\nBiology Science & Protein & 1 & 1 & 1 & 0 \\\\\nOther & Other Research & 1 & 1 & 0 & 1 \\\\ \\hline\n\\end{tabular}\n}\n\\end{table}\n\n\n\n\n\\section{Experiments}\n\n\n\\subsection{Preliminaries}\n\\subsubsection{Automatic Discovery Process}\nHere, we define \\textit{automatic discovery process} as the process by which an agent autonomously generates a series of scientific discoveries without human intervention~\\citep{clune2023automated}. During this process, agents will collaborate, refine their outputs, and enhance their performance. This process is depicted in Algorithm \\ref{alg:automatic}, where agents, given their initial instructions, generate outputs in each stage autonomously. In the $t$-th iteration, the \\textit{Judge Agent} will score the current output and discard the lowest scoring one. Agent's feedback and history information will be used to refine the outputs for the next iteration. \n\\begin{algorithm}\n\\caption{Automatic Discovery Process}\n\\label{alg:automatic}\n\\begin{algorithmic}[1]\n\\Require Agent \\Agent, \\Judge\n\\Require Initial instructions $\\mathcal{I}_{0}$.\n\\Ensure Discovery history $\\mathcal{H}_{0} \\leftarrow \\emptyset$, Agent feedback $\\mathcal{F}_{0} \\leftarrow \\emptyset$.\n\\For {$\\mathcal{T}$ iterations}\n    \\For {$\\mathcal{K}$ agents}\n        \\State $\\mathcal{O}_k \\leftarrow \\mathcal{A}_k\\mathcal{G}(\\mathcal{I}_k, \\mathcal{H}_{t-1}, \\mathcal{F}_{t-1})$. \\Comment{Outputs from agents}\n        \\State $\\mathcal{F}_k \\leftarrow \\mathcal{A}_k\\mathcal{G}(\\mathcal{I}_k, \\mathcal{H}_{t-1}, \\mathcal{F}_{t-1})$. \\Comment{Feedback from agents}\n    \\EndFor\n    \\State $\\mathcal{F}_{k^*), \\mathcal{O}_{k^*} \\leftarrow \\operatorname*{argmin}_{k} U(\\mathcal{O}_k, \\mathcal{J}_k, \\mathcal{F}_k, \\mathcal{I}_k)$. \\Comment{Choose the worst agent}\n    \\State $\\mathcal{O}_{k^*} \\leftarrow \\mathcal{O}_{k^*}\\cup \\{\\mathcal{O}_{k^*}^T\\}$. \\Comment{Add new paper to output}\n    \\State $\\mathcal{I}_{k^*}\\leftarrow \\mathcal{I}_{k^*}\\cup \\{\\mathcal{O}_{k^*}^T\\}$. \\Comment{Add new paper to instructions}\n    \\State $\\mathcal{H}_{t} \\leftarrow \\mathcal{H}_{t-1} \\cup \\{\\mathcal{O}_{k^*}^T\\}$. \\Comment{Add to discovery history}\n    \\State $\\mathcal{F}_{k^*} \\leftarrow \\mathcal{F}_{k^*} \\cup \\{\\mathcal{O}_{k^*}^T\\}$. \\Comment{Add new paper to feedback}\n    \\State \\textbf{do} $\\mathcal{A}_k\\mathcal{G}(\\mathcal{I}_k, \\mathcal{H}_{t}, \\mathcal{F}_{t})$. \\Comment{Rerieve the worst agent}\n    \\State $\\mathcal{F}_k \\leftarrow \\mathcal{A}_k\\mathcal{G}(\\mathcal{I}_k, \\mathcal{H}_{t}, \\mathcal{F}_{t})$. \\Comment{Feedback from agents}\n    \n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n\\subsubsection{Evaluation Metrics}\nThe first step in evaluating AI-generated content is determining what counts as high-quality generation~\\citep{NEURIPS2023_91f18a12,reid2024gemini}. In this paper, we use both automated methods and human evaluations to assess the quality of generated outputs, with a primary focus on three dimensions:\n\n\\begin{itemize}\n    \\item \\textit{Relevance and Novelty}: Whether the agent's outputs contain novel and significant contributions with respect to existing literature. The literature includes recent and related papers within a specific topic. \n    \\item \\textit{Soundness}: Whether there are obvious and major logical or factual errors in the agent's outputs.\n    \\item \\textit{Coherency}: Whether the reasoning provided by the agent's outputs is logical and consistent with itself and the given project. This dimension assesses the overall quality of writing, including the use of technical terms, citations, and scientific knowledge.\n\\end{itemize}\n\nAll the above dimensions need to be evaluated collectively. In some cases, an output that is sound but not very novel or coherent can still be a good output. The three dimensions need to be evaluated as a whole to give a comprehensive assessment of AI-generated content~\\citep{nuijten2016prevalence}. Therefore, we use both human evaluation and AI judge~\\citep{NEURIPS2020_6b493230,robertson2023gpt4,li2024ai4r} as our evaluation method. \n\n\\subsection{Inconsistent Performance}\nIn this section, we conduct experiments to investigate \"inconsistent performance\" using the LLM-as-judge method~\\citep{NEURIPS2020_6b493230,robertson2023gpt4}. We use GPT-4 to simulate literature research, problem formulation, and mathematical derivation in the target tasks. We use GPT-4 as a judge to judge whether the outputs generated by our method are better than those generated by agents. We sample from a large number of different research fields, which can give us a more convincing result to reveal the potential of GPT-4 in assisting scientific discovery. The results can be found in Table~\\ref{tab:human_study_7}.\n\\begin{table}[h]\n\\centering\n\\caption{Researchers' opinions on whether GPT-4 can assist in scientific discovery.}\n\\label{tab:human_study_7}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c|c|c}\n\\hline\n\\multirow{2}{*}{Groups} & \\multicolumn{4}{c|}{Researchers} & \\multicolumn{4}{c}{Others} \\\\ \\cline{2-9} \n& Others & Neuroscience & NLP & Molecular Biology & Others & Machine Learning & Cell Biology & Average & Variance \\\\ \\hline\nResearcher 1 & 750 & 683 & 660 & 690 & 640 & 662 & 696 & 674.3 & 18.5 \\\\\nResearcher 2 & 569 & 606 & 608 & 647 & 665 & 675 & 703 & 637.1 & 25.9 \\\\\nMaster & 681 & 645 & 690 & 703 & 698 & 741 & 751 & 693.3 & 20.5 \\\\\nDoctor & 687 & 676 & 691 & 709 & 671 & 671 & 716 & 691.1 & 14.3 \\\\\nResearch Group & 691 & 665 & 651 & 707 & 650 & 698 & 751 & 690.9 & 32.8 \\\\ \\hline\n\\end{tabular}\n}\n\\end{table}\n\nSpecifically, \\textbf{Researcher 1} is a professor from CS; \\textbf{Researcher 2} is an associate professor from ML; \\textbf{Master} is a master student from CV; \\textbf{Doctor} is a Ph.D. student from ML; and \\textbf{Research Group} includes four papers generated by our agents. The group \\textbf{Others} is the remaining. The seven examinee groups have various abilities to reveal the potential of GPT-4 in assisting scientific discovery. The numbers at the bottom of the table represent the number of queries for each group. The table details the scores from different groups and their variance is relatively large. The results suggest that the performance of the models varies significantly across different research fields. Moreover, we sample 100 papers generated by our methods and ask researchers and beginners to score them. The scores from CS, ML, CV, NLP, and Biology can be regarded as researchers, while the scores from other topics are regarded as beginners. The results show that the models perform best in molecular biology, followed by machine learning and neuroscience. Although the consistency of the outputs is relatively stable, there is still a lack of consistency in different research fields. \n\nFurthermore, the results show that the model has a reasonable understanding of science, but the model's performance in the machine learning and neuroscience fields is less helpful compared to other domains such as molecular biology. This is consistent with the findings of \\cite{hayes2024simulating}, which suggest that the model's ability to assist in the machine learning and neuroscience domains is relatively weak. \n\n\\subsection{Automatic Discovery Process}\n\nIn this section, we conduct experiments to demonstrate that the \\textit{singular} approach of agents to directly produce a scientific discovery is not effective. We found that the outputs of language models have relatively high consistency when given the same prompt. To investigate the performance of this method, we let the language agents conduct the whole process autonomously. The overall quality and consistency of the outputs are tested by asking GPT-4 to judge whether the output of research claim number two is better than research claim number one. The results are shown in \\cref{fig:performance}. It can be seen that the overall quality and consistency of the outputs are low and unstable. \n\n\\begin{wrapfigure}[16]{r}{0.4\\textwidth}\n    \\centering\n    \\includegraphics[width = 0.38\\linewidth]{original.pdf}\n    \\caption{Consistency and Validity}\n    \\label{fig:performance}\n\\end{wrapfigure}\n\n\\subsection{Autonomous Agents}\nAs shown in Figure~\\ref{fig:overview}, the autonomous agents can potentially lead to higher efficiency within the field of science, independently generating and and refining scientific discoveries. These agents are not designed to have the overall objective, e.g., novel contribution, and work independently. Mathematical derivation provides a deeper insight into data and phenomenon by translating them into mathematical language, this process helps to formalize research objectives, unify observations, and formalize data into something more consistent. It also helps to find errors and find different ways to achieve research objectives. \n\\begin{table}[h]\n\\centering\n\\caption{Performance of Autonomous Agents.}\n\\label{tab:agents}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|c|c|c|c}\n\\hline\n\\multicolumn{1}{l|}{Agents} & Start & Finish & Total Time & Cost \\\\ \\hline\nGPT-4 & 2023-09-23 11:21 & 2023-11-19 16:21 & 2 months & \\$ 7,410 \\\\ \\hline\nQwen2.5-14b & 2023-11-19 17:21 & 2023-12-23 09:21 & 2 weeks & \\$325 \\\\ \\hline\nLLaMA3.1-8b & 2023-11-19 11:21 & 2023-12-09 16:21 & 2 weeks & \\$185 \\\\ \\hline\n\\end{tabular}\n}\n\\end{table}\n\n\\begin{table}[]\n\\caption{Automatic Agents vs Human.}\n\\label{tab:agent_vs_human}\n\\centering\n\\resizebox{0.6\\linewidth}{!}{\n\\begin{tabular}{l|c}\n\\hline\nUsers & Avg. Score \\\\ \\hline\nResearchers & 5.39 \\\\ \\hline\nBeginners & 4.75 \\\\ \\hline\n\\end{tabular}\n}\n\\end{table}\n\n\\begin{table}[]\n\\caption{Automatic Agents vs Bot.}\n\\label{tab:agent_vs_bot}\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{l|l|l|l}\n\\hline\nTopics & Method & Score & \\#x \\\\ \\hline\n\\multirow{3}{*}{\\makecell[l]{Vision\\&Imaging}} & GPT-4 & 6.21 & 54.9 \\\\\n & Qwen2.5 & 6.46 & 5 \\\\ \n & AIScientist & 7.12 & P \\\\ \\hline\n\\multirow{2}{*}{NLP} & GPT-4 & 6.58 & 50 \\\\\n & Qwen2.5 & 6.89 & 48.3 \\\\ \\hline\n\\multirow{2}{*}{Language\\&Speech} & GPT-4 & 6.58 & 50 \\\\\n & Qwen2.5 & 6.16 & 40.4 \\\\ \\hline\n\\multirow{2}{*}{Machine Learning} & GPT-4 & 6.18 & 51.7 \\\\\n & Qwen2.5 & 5.99 & 35.1 \\\\ \\hline\n\\end{tabular}\n}\n\\end{table}\n\n\\subsection{Results}\\label{sec:results}\nIn this section, we conduct experiments to demonstrate the performance of autonomous agents. We apply ScientificSniper to 21 topics, collecting a total of 152 project descriptions (each project may have several descriptions) and using them as inputs to the system. The average scores are shown in \\cref{tab:agents}. It can be found that the results suggest that we can achieve a comparable level of output quality with that of a human. In addition, as shown in \\cref{tab:agents}, a larger model can produce better, more reasonable results, but this does not mean that smaller models cannot be used to write good papers. As shown in \\cref{tab:agents}, we have deployed two models, including a smaller LLaMA3.1-8b model and a larger GPT-4 model. The LLaMA3.1-8b model, though smaller in size, can produce good results with a higher efficiency. The GPT-4 model, however, while demonstrating exceptional performance in both quality and efficiency, incurs significantly higher financial costs. These findings highlight the trade-offs between model size, performance, and resource utilization in the context of scientific discovery. Our approach suggests that it is possible to achieve close to state-of-the-art results without deploying large-scale models, thereby offering a more efficient and cost-effective solution.\n\nIn addition, we conducted an extensive experiment to demonstrate that our system is generally regarded as \"\\textit{not bad}\" by both researchers and beginners. We have evaluated the papers by 100 users and the user scores are shown in \\cref{tab:agent_vs_human}. Among them, 20 are researchers in CS, ML, CV, NLP, and Biology, while the remaining 80 are beginners. The user can score the paper from 1 to 10. The results show that the average score of researchers is higher than that of beginners, suggesting that our papers are more accepted by experts in the field, although they have some deficiencies. In contrast, if the outputs are generated by GPT-4 directly, our system will have a similar score as GPT-4, but higher than GPT-4 without project description as input.\n\nThe system can achieve a relatively high level of performance in some specific topics. We conducted experiments to demonstrate that our system can assist researchers with writing scientific papers. We use AI4Science~\\citep{ai4science2023impact} to write a research paper against topics as the baseline method. The topics are selected from GPT-4 and integrate potential research directions in the future. We apply both GPT-4 and Qwen2.5 to apply this method. In addition, we apply our methods to the same topic and compare the results. The results are shown in \\cref{tab:agent_vs_bot}. It can be found that, compared with the method of randomly picking a topic and directly asking GPT-4 to write a paper, our system can achieve a higher score as well as more detailed content.\n\n\n\n\\section{Conclusion}\n\nThis paper presents ScientificSniper, a collaborative multi-agent system designed to emulate the incremental processes of scientific discovery. It includes a team of six agents, each specialized in different aspects of the research process. Together, they have successfully automated the entire process of a multi-disciplinary discovery task, providing a high-level abstraction of literature, research problem, mathematical derivation and validation, with each discovery iteratively backed by theoretical analysis, and used effectively in re-training the agents with a greater amount of data. These agents work independently and are not equipped with an overall objective, e.g., novel contribution, and work independently. Mathematical derivation provides a deeper insight into data and phenomenon, helping to formalize research objectives, unify observations, and formalize data into something more consistent. It also helps to find errors and find different ways to achieve research objectives.\nFor future research, we may further generalize the system to be more autonomous by automatically selecting topics, as well as agentically reviewing the papers~\\citep{tan2024peerreviewmultiturnlongcontext,tyser2024ai,d2024marg,baek2024researchagent}. In addition, we may further\n"
}